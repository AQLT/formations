[
  {
    "objectID": "2022/ast/TP/1_Manipulation_series_temporelles.html",
    "href": "2022/ast/TP/1_Manipulation_series_temporelles.html",
    "title": "1 - Traitement des séries temporelles sous R",
    "section": "",
    "text": "L’objectif de ce TP est d’apprendre à manipuler les séries temporelles sous R en utilisant les packages de bases.\nDans R il existe de nombreux packages qui permettent de manipuler les séries temporelles. Pour n’en citer que quelques-uns :\n- Les objets ts peuvent être créés à partir du package stats ;\n- Les objets zoo peuvent être créés à partir du package zoo ;\n- Les objets xts peuvent être créés à partir du package xts ;\n- Les objets tis peuvent être créés à partir du package tseries ;\n- Les objets tsibble peuvent être créés à partir du package tsibble.\ntsbox permet quand à lui de facilement passer d’une classe à l’autre.\nIci nous nous concentrerons essentiellement sur les trois premiers : ts stocker les séries temporelles, zoo et xts pour effectuer certaines manipulations supplémentaires.\nLes packages suivants seront utilisés :"
  },
  {
    "objectID": "2022/ast/TP/1_Manipulation_series_temporelles.html#création-dune-série-temporelle",
    "href": "2022/ast/TP/1_Manipulation_series_temporelles.html#création-dune-série-temporelle",
    "title": "1 - Traitement des séries temporelles sous R",
    "section": "Création d’une série temporelle",
    "text": "Création d’une série temporelle\nLa fonction ts() permet de créer des objets séries-temporelles à partir un vecteur (ou une matrice). La syntaxe de base est ts(vector, start=, end=, frequency=) où start et end sont la première et la dernière observation, frequency est le nombre d’observations par unité de temps (1=annuelle, 2=semestrielle, 4=trimestrielle, 6=bi-mestrielle, 12=mensuelle, etc.).\nPar exemple pour créer une série trimestrielle ayant les valeurs de 1 à 10 et commençant en 1959Q2 :\n\nts(1:10, frequency = 4, start = c(1959, 2)) # 2ème trimestre de 1959\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10     \n\n# Équivalent à \nts(1:10, frequency = 4, start = 1959 + 1/4)\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10     \n\n\nOn peut aussi définir l’objet à partir de sa date de fin :\n\nts(1:10, frequency = 4, end = c(1959, 2))\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1957    1    2    3    4\n1958    5    6    7    8\n1959    9   10          \n\n\nSi l’on directement extraire un sous-ensemble de la série on peut spécifier les paramètres end et start. Par exemple pour ne garder que les valeurs jusqu’en 1960 inclus :\n\nts(1:10, frequency = 4, start = c(1959, 2), end = c(1960, 4))\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n\n\nOu alors utiliser la fonction window une fois l’objet créé :\n\nts_object <- ts(1:10, frequency = 4, start = c(1959, 2))\nwindow(ts_object, end = c(1960, 4))\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n\n\nOn peut récupérer les différents attributs avec les fonctions start(), end() et frequency() :\n\nstart(ts_object)\n\n[1] 1959    2\n\nend(ts_object)\n\n[1] 1961    3\n\nfrequency(ts_object)\n\n[1] 4\n\n\nDeux autres fonctions peuvent aussi être utiles : time() crée un série-temporelle à partir des dates de notre série-temporelle et cycle() donne la position dans le cycle de chaque observation.\n\ntime(ts_object)\n\n        Qtr1    Qtr2    Qtr3    Qtr4\n1959         1959.25 1959.50 1959.75\n1960 1960.00 1960.25 1960.50 1960.75\n1961 1961.00 1961.25 1961.50        \n\ncycle(ts_object)\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         2    3    4\n1960    1    2    3    4\n1961    1    2    3     \n\n\n\n\n\n\n\n\nExercice\n\n\n\nExtraire toutes les données du 2ème trimestre de l’objet ts_object\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nts_object[cycle(ts_object) == 2]\n\n[1] 1 5 9\n\n\nAutre option : utiliser la fonction window()\n\nwindow(ts_object, frequency = 1)\n\nTime Series:\nStart = 1959.25 \nEnd = 1961.25 \nFrequency = 1 \n[1] 1 5 9\n\n\nExplication : lorsque l’on spécifie le paramètre frequency dans la fonction window(), on change la fréquence de la série. Dans notre cas, on veut extraire les valeurs du deuxième trimestre : on veut donc une série annuelle qui contient toutes les valeurs des deuxièmes trimestres. La première observation de ts_object étant un deuxième trimestre, cela donne ce que l’on veut. Pour extraire les valeurs des troisièmes trimestres il faut en plus changer la date de début :\n\nwindow(ts_object, start = c(1950, 3), frequency = 1)\n\nTime Series:\nStart = 1959.25 \nEnd = 1961.25 \nFrequency = 1 \n[1] 1 5 9\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nCréer une série temporelle mensuelle qui commence en 2000, qui se termine en janvier 2020, qui vaut 1 en avril 2009 et 0 à toutes les autres dates.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOption 1 : utiliser la fonction window()\n\nindicatrice <- ts(0, start = 2000, end = 2020, frequency = 12)\nwindow(indicatrice, start = c(2009, 4), end = c(2009, 4)) <- 1\nindicatrice\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2000   0   0   0   0   0   0   0   0   0   0   0   0\n2001   0   0   0   0   0   0   0   0   0   0   0   0\n2002   0   0   0   0   0   0   0   0   0   0   0   0\n2003   0   0   0   0   0   0   0   0   0   0   0   0\n2004   0   0   0   0   0   0   0   0   0   0   0   0\n2005   0   0   0   0   0   0   0   0   0   0   0   0\n2006   0   0   0   0   0   0   0   0   0   0   0   0\n2007   0   0   0   0   0   0   0   0   0   0   0   0\n2008   0   0   0   0   0   0   0   0   0   0   0   0\n2009   0   0   0   1   0   0   0   0   0   0   0   0\n2010   0   0   0   0   0   0   0   0   0   0   0   0\n2011   0   0   0   0   0   0   0   0   0   0   0   0\n2012   0   0   0   0   0   0   0   0   0   0   0   0\n2013   0   0   0   0   0   0   0   0   0   0   0   0\n2014   0   0   0   0   0   0   0   0   0   0   0   0\n2015   0   0   0   0   0   0   0   0   0   0   0   0\n2016   0   0   0   0   0   0   0   0   0   0   0   0\n2017   0   0   0   0   0   0   0   0   0   0   0   0\n2018   0   0   0   0   0   0   0   0   0   0   0   0\n2019   0   0   0   0   0   0   0   0   0   0   0   0\n2020   0                                            \n\n\nOption 2 : utiliser time()\n\nindicatrice <- ts(0, start = 2000, end = 2020, frequency = 12)\n# Donne un vecteur de booléens\n(time(indicatrice) == 2009 + 3/12) \n\n       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n2000 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2001 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2002 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2003 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2004 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2005 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2006 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2007 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2008 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2009 FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2010 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2011 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2012 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2013 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2014 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2015 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2016 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2017 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2018 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2019 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n2020 FALSE                                                                  \n\n# on ajoute + 0 pour forcer la convertion en numérique\n(time(indicatrice) == 2009 + 3/12) + 0\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2000   0   0   0   0   0   0   0   0   0   0   0   0\n2001   0   0   0   0   0   0   0   0   0   0   0   0\n2002   0   0   0   0   0   0   0   0   0   0   0   0\n2003   0   0   0   0   0   0   0   0   0   0   0   0\n2004   0   0   0   0   0   0   0   0   0   0   0   0\n2005   0   0   0   0   0   0   0   0   0   0   0   0\n2006   0   0   0   0   0   0   0   0   0   0   0   0\n2007   0   0   0   0   0   0   0   0   0   0   0   0\n2008   0   0   0   0   0   0   0   0   0   0   0   0\n2009   0   0   0   1   0   0   0   0   0   0   0   0\n2010   0   0   0   0   0   0   0   0   0   0   0   0\n2011   0   0   0   0   0   0   0   0   0   0   0   0\n2012   0   0   0   0   0   0   0   0   0   0   0   0\n2013   0   0   0   0   0   0   0   0   0   0   0   0\n2014   0   0   0   0   0   0   0   0   0   0   0   0\n2015   0   0   0   0   0   0   0   0   0   0   0   0\n2016   0   0   0   0   0   0   0   0   0   0   0   0\n2017   0   0   0   0   0   0   0   0   0   0   0   0\n2018   0   0   0   0   0   0   0   0   0   0   0   0\n2019   0   0   0   0   0   0   0   0   0   0   0   0\n2020   0                                            \n\n\n\n\n\nPour tracer un graphique il suffit maintenant d’utiliser les fonctions plot() et lines()\n\nplot(ts_object * 2)\nlines(ts_object, col = \"red\")"
  },
  {
    "objectID": "2022/ast/TP/1_Manipulation_series_temporelles.html#séries-multivariées",
    "href": "2022/ast/TP/1_Manipulation_series_temporelles.html#séries-multivariées",
    "title": "1 - Traitement des séries temporelles sous R",
    "section": "Séries multivariées",
    "text": "Séries multivariées\nDe la même façon que précédemment on peut créer une série temporelle multivariée. Cette fois-ci l’objet créé est à la fois mts, ts et matrix\n\nset.seed(1)\n# On génère 300 observations d'une loi normale (0, 1)\nloi_normale <- rnorm(300)\nmts <- ts(matrix(loi_normale, nrow = 100, ncol = 3),\n          start = c(1961, 1), frequency = 12)\n\nOn peut accéder à la première variable de la même façon que dans une matrice : par son nom ou son numéro de colonne :\n\ncolnames(mts)\n\n[1] \"Series 1\" \"Series 2\" \"Series 3\"\n\n# mts[,1] # ou de façon équivalente :\nmts[, \"Series 1\"]\n\n              Jan          Feb          Mar          Apr          May\n1961 -0.626453811  0.183643324 -0.835628612  1.595280802  0.329507772\n1962 -0.621240581 -2.214699887  1.124930918 -0.044933609 -0.016190263\n1963  0.619825748 -0.056128740 -0.155795507 -1.470752384 -0.478150055\n1964 -0.394289954 -0.059313397  1.100025372  0.763175748 -0.164523596\n1965 -0.112346212  0.881107726  0.398105880 -0.612026393  0.341119691\n1966  2.401617761 -0.039240003  0.689739362  0.028002159 -0.743273209\n1967  0.610726353 -0.934097632 -1.253633400  0.291446236 -0.443291873\n1968  0.593946188  0.332950371  1.063099837 -0.304183924  0.370018810\n1969 -1.276592208 -0.573265414 -1.224612615 -0.473400636             \n              Jun          Jul          Aug          Sep          Oct\n1961 -0.820468384  0.487429052  0.738324705  0.575781352 -0.305388387\n1962  0.943836211  0.821221195  0.593901321  0.918977372  0.782136301\n1963  0.417941560  1.358679552 -0.102787727  0.387671612 -0.053805041\n1964 -0.253361680  0.696963375  0.556663199 -0.688755695 -0.707495157\n1965 -1.129363096  1.433023702  1.980399899 -0.367221476 -1.044134626\n1966  0.188792300 -1.804958629  1.465554862  0.153253338  2.172611670\n1967  0.001105352  0.074341324 -0.589520946 -0.568668733 -0.135178615\n1968  0.267098791 -0.542520031  1.207867806  1.160402616  0.700213650\n1969                                                                 \n              Nov          Dec\n1961  1.511781168  0.389843236\n1962  0.074564983 -1.989351696\n1963 -1.377059557 -0.414994563\n1964  0.364581962  0.768532925\n1965  0.569719627 -0.135054604\n1966  0.475509529 -0.709946431\n1967  1.178086997 -1.523566800\n1968  1.586833455  0.558486426\n1969                          \n\n\nEt avec les même fonctions que pour les matrices on peut récupérer les noms des colonnes (colnames), le nombre de variables (ncol), etc.\n\n\n\n\n\n\nAttention\n\n\n\nUne source classique d’erreur est de manipuler des séries-temporelles uni et multivariées et de vouloir utiliser les fonctions liées aux matrices sur les séries univariées. Par exemple, colnames(ts_object) renverra toujours l’objet NULL. Une solution est de tester si l’objet est multivarié avec la fonction is.mts()."
  },
  {
    "objectID": "2022/ast/TP/1_Manipulation_series_temporelles.html#manipulation-basiques",
    "href": "2022/ast/TP/1_Manipulation_series_temporelles.html#manipulation-basiques",
    "title": "1 - Traitement des séries temporelles sous R",
    "section": "Manipulation basiques",
    "text": "Manipulation basiques\nPour concaténer plusieurs séries temporelles, les fonctions deux fonctions suivantes peuvent ts.union() et ts.intersect().\n\nts_object2 <- ts(1:10, frequency = 4, start = c(1960, 1))\nts.union(ts_object, ts_object2) # on garde toute la couverture temporelle en rajoutant des NA\n\n        ts_object ts_object2\n1959 Q2         1         NA\n1959 Q3         2         NA\n1959 Q4         3         NA\n1960 Q1         4          1\n1960 Q2         5          2\n1960 Q3         6          3\n1960 Q4         7          4\n1961 Q1         8          5\n1961 Q2         9          6\n1961 Q3        10          7\n1961 Q4        NA          8\n1962 Q1        NA          9\n1962 Q2        NA         10\n\nts.intersect(ts_object, ts_object2) # on ne garde que les périodes communes\n\n        ts_object ts_object2\n1960 Q1         4          1\n1960 Q2         5          2\n1960 Q3         6          3\n1960 Q4         7          4\n1961 Q1         8          5\n1961 Q2         9          6\n1961 Q3        10          7\n\n\nOn va maintenant utiliser la série d’indice de production industrielle de la France (CVS-CJO) :\n\nipi_fr_manuf <- ts(c(99, 99.4, 99.7, 99.4, 100.8, 100, 98.7, 100.2, 101.2, \n100.6, 99.9, 100.9, 102.4, 100.8, 99.5, 100.7, 99.8, 99.1, 99.8, \n101.6, 100.4, 99.4, 102.8, 101, 100.2, 101.1, 102.6, 101.8, 103.7, \n103, 103.6, 103.5, 104.4, 105.6, 105.5, 105.9, 103.6, 102.9, \n103.8, 103.8, 102.5, 104.2, 104, 104.6, 103.4, 104.2, 103.4, \n103.7, 104.9, 105.8, 104.4, 104.3, 106, 103.7, 104.1, 103.1, \n103.9, 104.4), start = 2015, frequency = 12)\n\nPour calculer la série retardée/avancée, il suffit d’utiliser la fonction lag() :\n\n# série retardée d'un mois : en février 2010 on a la valeur de janvier 2010\nlag(ipi_fr_manuf, k = -1) \n\nLa fonction diff permet de calculer la différence entre deux périodes\n\ndiff(ipi_fr_manuf, k = 1)\n\n\n\n\n\n\n\nExercice\n\n\n\nÉcrire une fonction ev() qui calcule l’évolution mensuelle si la série en entrée est mensuelle, l’évolution trimestrielle si la série en entrée est trimestrielle, etc.\nLa fonction ev() transformera donc toute série \\(X_t\\) en : \\[\nY_t=\\frac{X_t-X_{t-1}}{\nX_{t-1}\n}=\\frac{X_t}{\nX_{t-1}\n} - 1\n\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nev <- function(x){\n  result <- (x/stats::lag(x, k = -1) - 1) * 100\n  return(result)\n}\n# Ou de manière équivalente :\nev2 <- function(x){\n  # Attention ici c'est bien k = 1 dans la fonction diff\n  # et k = -1 dans la fonction lag\n  result <- (diff(x, k = 1) /lag(x, k = -1)) * 100\n  return(result)\n}\n\nRemarque : pour des raisons informatiques ces deux fonctions ne donnent pas exactement le même résultat. C’est un problème récurrent lorsque l’on souhaite tester l’égalité entre deux séries temporelles :\n\nall(ev(ipi_fr_manuf) == ev2(ipi_fr_manuf))\n\n[1] FALSE\n\n\nUne solution est plutôt d’utiliser la fonction all.equal() :\n\nisTRUE(all.equal(ev(ipi_fr_manuf), ev2(ipi_fr_manuf)))\n\n[1] TRUE"
  },
  {
    "objectID": "2022/ast/TP/1_Manipulation_series_temporelles.html#utilisation-de-xts",
    "href": "2022/ast/TP/1_Manipulation_series_temporelles.html#utilisation-de-xts",
    "title": "1 - Traitement des séries temporelles sous R",
    "section": "Utilisation de xts",
    "text": "Utilisation de xts\nUn des avantages du package xts est qu’il permet d’appliquer une fonction à chaque période d’une série temporelle (par exemple à toutes les données trimestrielles, annuelles, etc.). Il s’agit des fonctions apply.monthly(), apply.quarterly(), apply.yearly(), etc. Pour cela il faut auparavant convertir les données au format xts.\nPar exemple pour calculer la moyenne annuelle :\n\nlibrary(xts)\nmoy_an <- apply.yearly(as.xts(ipi_fr_manuf), mean)\nmoy_an\n\n              [,1]\ndéc 2015  99.98333\ndéc 2016 100.60833\ndéc 2017 103.40833\ndéc 2018 103.67500\noct 2019 104.46000\n\n\n\n\n\n\n\n\nExercice\n\n\n\nCalculer l’évolution trimestrielle de ipi_fr_manuf.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTout d’abord on prolonge l’IPI par des valeurs manquantes jusqu’à la fin de l’année, sinon la dernière somme sur le trimestre est fausse.\n\nipi_fr_manuf_prolonge <- window(ipi_fr_manuf, end = c(2019, 12), extend = TRUE)\nsomme_trim <- apply.quarterly(as.xts(ipi_fr_manuf_prolonge), sum)\n\nAttention la fonction lag n’agit pas pareil pour les objets xts et ts : il faut ici utiliser l’option k = 1. Voir l’aide associée à ?lag.xts. Pour garder la même convention entre lag.ts() et lag.xts() on peut utiliser l’option options(xts.compat.zoo.lag=TRUE).\n\nevol_trim <- (somme_trim/lag(somme_trim, k = 1) - 1) * 100\n\nOn peut utiliser la fonction format() si l’on veut convertir automatiquement en un objet ts :\n\nstart_year <- as.numeric(format(start(evol_trim), \"%Y\"))\nstart_quarter <- as.numeric(substr(quarters(start(evol_trim)), 2, 2))\nts(evol_trim, start = c(start_year, start_quarter), frequency = 4)\n\n            Qtr1        Qtr2        Qtr3        Qtr4\n2015          NA  0.70446159 -0.03331113  0.43318894\n2016  0.43132050 -1.02411629  0.73431242  0.46388337\n2017  0.23087071  1.51365581  0.97244733  1.76565008\n2018 -2.11356467  0.06445375  0.48309179 -0.22435897\n2019  1.22068744 -0.34909553 -0.92356688          NA\n\n\nOn peut aussi directement utiliser le package ts_box et la fonction ts_ts() :\n\ntsbox::ts_ts(evol_trim)\n\n            Qtr1        Qtr2        Qtr3        Qtr4\n2015                      NA  0.70446159 -0.03331113\n2016  0.43318894  0.43132050 -1.02411629  0.73431242\n2017  0.46388337  0.23087071  1.51365581  0.97244733\n2018  1.76565008 -2.11356467  0.06445375  0.48309179\n2019 -0.22435897  1.22068744 -0.34909553 -0.92356688\n2020          NA                                    \n\n\n\n\n\nOn aurait en fait pu le faire directement avec les fonctions de base R ! Par contre la situation aurait été plus compliquée avec des données haute fréquence (du type journalières) non gérées par ts :\n\naggregate(ipi_fr_manuf, nfrequency = 4,\n          FUN = mean)\n\n          Qtr1      Qtr2      Qtr3      Qtr4\n2015  99.36667 100.06667 100.03333 100.46667\n2016 100.90000  99.86667 100.60000 101.06667\n2017 101.30000 102.83333 103.83333 105.66667\n2018 103.43333 103.50000 104.00000 103.76667\n2019 105.03333 104.66667 103.70000"
  },
  {
    "objectID": "2022/ast/TP/1_Manipulation_series_temporelles.html#utilisation-de-zoo",
    "href": "2022/ast/TP/1_Manipulation_series_temporelles.html#utilisation-de-zoo",
    "title": "1 - Traitement des séries temporelles sous R",
    "section": "Utilisation de zoo",
    "text": "Utilisation de zoo\nLe package zoo donne un ensemble d’outils qui permettent de manipuler les séries-temporelles. De nombreux packages (dont xts) sont d’ailleurs basés sur ce format. Il permet notamment de faire des imputations de données manquantes selon différentes fonctions (toutes les fonctions commençant par na.) et de mieux gérer le format des dates associées aux séries temporelles (ce qui permet de faire des manipulations avec la fonction format, ce qui permet par exemple plus facilement exporter des séries temporelles sous Excel). Le calcul de l’évolution trimestrielle aurait par exemple pu être faite avec ce package :\n\nsomme_trim <- aggregate(as.zoo(ipi_fr_manuf_prolonge), yearqtr, sum)\nsomme_trim <- as.ts(somme_trim) #La conversion en ts est plus simple depuis un objet zoo\nevol_trim <- ev(somme_trim)\nevol_trim\n\n            Qtr1        Qtr2        Qtr3        Qtr4\n2015              0.70446159 -0.03331113  0.43318894\n2016  0.43132050 -1.02411629  0.73431242  0.46388337\n2017  0.23087071  1.51365581  0.97244733  1.76565008\n2018 -2.11356467  0.06445375  0.48309179 -0.22435897\n2019  1.22068744 -0.34909553 -0.92356688          NA\n\n\nPour le prochain exercice, utiliser la série suivante :\n\nserie_avec_NA <- ts(c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, \n  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, \n  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \n  NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, NA, NA, NA, NA, NA, \n  NA, NA, NA, NA, NA, NA), start = 2000, frequency = 12)\n\n\n\n\n\n\n\nExercice\n\n\n\nSur la série serie_avec_NA, utiliser les différentes fonctions du package zoo pour :\n\nEnlever les valeurs manquantes au début de la série ;\n\nRemplacer les valeurs manquantes à la fin de la série par la dernière valeur observée.\n\nInterpoler de manière linéaire les valeurs manquantes entre les 0 et les 1.\n\n\n\n\n\n\n\n\n\nIndice\n\n\n\n\n\nLes trois fonctions à utiliser sont : na.trim(), na.locf et na.approx(). Il faudra peut-être inverser deux étapes pour que cela marche.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nD’abord on enlève les valeurs manquantes au début de la série\n\netape_1 <- na.trim(serie_avec_NA, sides = \"left\")\netape_1\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2001   0   0   0   0   0   0   0   0   0   0   0   0\n2002   0   0   0   0   0   0   0   0   0   0   0   0\n2003  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n2004  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n2005   1   1   1   1   1   1   1   1   1   1   1   1\n2006   1   1   1   1   1   1   1   1   1   1   1   1\n2007  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n\n\nEnsuite on interpole\n\netape_2 <- na.approx(etape_1, na.rm = FALSE)\netape_2\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n2001 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2002 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2003 0.04 0.08 0.12 0.16 0.20 0.24 0.28 0.32 0.36 0.40 0.44 0.48\n2004 0.52 0.56 0.60 0.64 0.68 0.72 0.76 0.80 0.84 0.88 0.92 0.96\n2005 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n2006 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n2007   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA\n\n\nEnfin on remplace les valeurs à la fin de la série\n\netape_3 <- na.locf(etape_2)\netape_3\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n2001 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2002 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2003 0.04 0.08 0.12 0.16 0.20 0.24 0.28 0.32 0.36 0.40 0.44 0.48\n2004 0.52 0.56 0.60 0.64 0.68 0.72 0.76 0.80 0.84 0.88 0.92 0.96\n2005 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n2006 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n2007 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nÀ l’aide des fonctions as.yearmon() et format(), créer un data.frame contenant une colonne “date” qui contient les dates au format JJ/MM/YYYY et une deuxième colonnes avec les valeurs de ipi_fr_manuf.\n\n\n\n\n\n\n\n\nIndice\n\n\n\n\n\nLa fonction as.yearmon() doit être appliquée sur time(ipi_fr_manuf). Pour la fonction format regarder l’aide ?format.Date.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndates <- as.yearmon(time(ipi_fr_manuf))\ndates <- format(dates, \"%d/%m/%Y\")\ndonnees_formatees <- data.frame(date = dates, ipi = ipi_fr_manuf)\nhead(donnees_formatees)\n\n        date   ipi\n1 01/01/2015  99.0\n2 01/02/2015  99.4\n3 01/03/2015  99.7\n4 01/04/2015  99.4\n5 01/05/2015 100.8\n6 01/06/2015 100.0\n\n\n\n\n\nIl peut également être utile d’exporter un objet R ts ou mts vers un fichier Excel, tout en rajoutant une colonne “date” qui sera au format date. Ci-dessous un exemple en utilisant le package XLConnect :\n\nlibrary(XLConnect)\nts2xls <- function(x, file, sheet=\"Feuille 1\", format = \"dd/mm/yyyy\"){\n  wb <- loadWorkbook(file, create = TRUE)\n  createSheet(wb, sheet)\n  if(is.mts(x)){\n    col <- c(\"date\", colnames(x))\n  }else{\n    col <- c(\"date\", \"x\")\n  }\n  # Le titre\n  writeWorksheet(wb,matrix(col,nrow = 1),\n                 sheet = sheet,startCol = 1,startRow =1,\n                 header = FALSE)\n\n  # Petit trick pour que la colonne date soit au format date d'Excel\n  csDate <- getOrCreateCellStyle(wb, name = \"date\")\n  setDataFormat(csDate, format = format)\n  date <- as.Date(format(zoo::as.Date((time(x))), \"%d/%m/%Y\"),\n                  \"%d/%m/%Y\")\n  writeWorksheet(wb,date,sheet = sheet,\n                 startCol = 1,startRow = 2,\n                 header = FALSE)\n  setCellStyle(wb, sheet = sheet, row = seq_along(date)+1,\n               col = 1,\n               cellstyle = csDate)\n  # Fin colonne date\n\n  # Autres colonnes\n  writeWorksheet(wb,x,sheet = sheet,startCol = 2,startRow = 2,\n                 header = FALSE)\n  setColumnWidth(wb, sheet, column = seq_along(col), width = -1)\n  saveWorkbook(wb, file)\n}"
  },
  {
    "objectID": "2022/ast/TP/2_Graphiques.html",
    "href": "2022/ast/TP/2_Graphiques.html",
    "title": "2 - Analyse graphique",
    "section": "",
    "text": "L’objectif de ce TP est d’approfondir les analyses graphiques et les transformations pour stabiliser la variance.\nLes packages suivants seront utilisés :"
  },
  {
    "objectID": "2022/ast/TP/2_Graphiques.html#analyse-graphique-temporelle",
    "href": "2022/ast/TP/2_Graphiques.html#analyse-graphique-temporelle",
    "title": "2 - Analyse graphique",
    "section": "Analyse graphique temporelle",
    "text": "Analyse graphique temporelle\nLe but des prochains exercices est de mettre en pratique les différentes fonctions pour tracer les graphiques vues en cours.\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant la fonction help(), décrire les trois séries temporelles gold, woolyrnq et gas du package forecast. Quelles sont les périodicités des séries ?\nTracer les 3 séries. Repérer le point atypique sur la série gold : à quelle observation apparaît-il ? À quelle date cela correspond ?\n\n\n\n\n\n\n\n\nIndice\n\n\n\n\n\nOn pourra utiliser la fonction which.max() pour repérer le point atypique. La série gold correspond à la série journalière des prix de l’or hors vendredi samedi : pour récupérer les jours correspondants aux numéros des observations, on pourra utiliser la fonction seq.Date() ainsi que la fonction lubridate::wday().\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngold représente les prix journaliers de l’or en dollar (hors vendredi et samedi) entre le 1er janvier 1985 et le 31 Mars 1989. Il semble y avoir des valeurs manquantes, un point atypique autour de l’observation 750. Une tendance plutôt à la hausse jusqu’à cette observation puis une tendance à la baisse. Pas de saisonnalité visible\n\nautoplot(gold, y = \"Prix en $\",\n         main = \"Prix journaliers de l'or\")\n\n\n\n\nwoolyrnq représente la production trimestrielle de laine en Australie (en tonnes) entre le premier trimestre 1965 et le troisième trimestre 1994.\n\nautoplot(woolyrnq, y = \"tonnes\",\n         main = \"Production trimestrielle de laine en Australie\")\n\n\n\n\ngas représente la production mensuelle de gaz en Australie entre janvier 1956 et août 1995\n\nautoplot(gas, main = \"Production mensuelle de gaz en Australie\")\n\n\n\n\n\n# Point atypique net pour la série gold à l'observation 770\nwhich.max(gold)\n\n[1] 770\n\ngold[which.max(gold)]\n\n[1] 593.7\n\ndates = seq(from = ymd(\"1985-01-01\"), to = ymd(\"1989-03-31\"), by = \"day\")\nhors_wd = dates[lubridate::wday(dates,week_start = 1) %in% c(1:4,7)] # on ne prend pas les jours 6 et 7 (samedi et dimanche)\nlength(hors_wd) - length(gold) # on a bien le même nombre d'observations\n\n[1] 0\n\nhors_wd[which.max(gold)]\n\n[1] \"1987-12-14\"\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nAnalyser les séries de fpp3::aus_arrivals (arrivées internationales trimestrielles en Australie) : évolution, tendance, saisonnalité et points atypiques (autoplot(), gg_season() et gg_subseries()).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\naus_arrivals %>% autoplot(Arrivals)\n\n\n\n\nLe nombre d’arrivées augmentent avec le temps, sauf ceux en provenance du Japon après 1995. Les séries semblent saisonnières avec une saisonnalité qui dépend du niveau (sauf pour US) et on observe un changement de saisonnalité pour les arrivées en provenance du Japon\n\naus_arrivals %>% gg_season(Arrivals)\n\n\n\n\nRemarque : le multivarié ne marche pas avec forecast. forecast::ggseasonplot(arrivals) donne une erreur.\nLa saisonnalité est différente entre chaque pays : arrivées plus élevées aux T1 et T4 pour le Royaume-Uni. elles sont plus faibles au T1 pour la Nouvelle-Zélande et au plus haut au T3. Pour le Japon : plus faibles aux T2 et T4 sur années récentes. Pour les États-Unis le graphique n’est pas facile à lire\n\naus_arrivals %>% gg_subseries(Arrivals)\n\n\n\n\nPour le Royaume-Uni la hausse des entrées est surtout saisonnière (forte hausse aux T1 et T3). Depuis les États-Unis et la Nouvelle-Zélande la hausse semble la même sur tous les trimestres.\nPlusieurs points atypiques s’observent, par exemple :\n\n2000T3 pour les US (JO)\n\n2001T3-T4 pour les US (11 septembre)\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant les différentes fonctions apprises pour tracer les graphiques (autoplot(), ggseasonplot() et ggsubseriesplot(), gglagplot() et ggAcf()) analyser la série fma::hsales (tendance, saisonnalité, cycle, points atypiques).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nautoplot(hsales)\n\n\n\n\nPas de tendance, il semble il y avoir une saisonnalité et un cycle\n\nggseasonplot(hsales)\n\n\n\n\nLes ventes semblent plus faibles en janv-déc et plus élevées en mars\n\nggsubseriesplot(hsales)\n\n\n\n\nOn retrouve en moyenne les résultats précédents mais les variations des coefficients saisonniers laissent penser à un cycle\n\ngglagplot(hsales)\n\n\n\n\nForte corrélation avec les valeurs précédentes et les valeurs saisonnières\n\nggAcf(hsales)\n\n\n\nggAcf(hsales, 12*7)\n\n\n\n\nEn augmentant le nombre de lag on repère plus facilement les cycles longs (environ 8 ans)."
  },
  {
    "objectID": "2022/ast/TP/2_Graphiques.html#densité-spectrale",
    "href": "2022/ast/TP/2_Graphiques.html#densité-spectrale",
    "title": "2 - Analyse graphique",
    "section": "Densité spectrale",
    "text": "Densité spectrale\nLe but de ces exercices est de calculer les densités spectrales des différentes composantes :\n\ntendance\n\ncycle\n\nsaisonnalité\n\nirrégulier\n\nAttention : par défaut la fonction spectrum enlève une tendance linéaire à la série ! Voir l’aide associée à la fonction ?spectrum.\n\n\n\n\n\n\nExercice\n\n\n\nCalculer le périodogramme et le spectre autorégressif d’une tendance.\n\n\n\n\n\n\n\n\nIndice\n\n\n\n\n\nIl faut utiliser les paramètres spectrum(., detrend = FALSE, log = \"no\").\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nt = ts(1:100, frequency = 12, start = 2000)\nspectrum(t, \n         detrend = FALSE,\n         method = \"pgram\", log = \"no\")\n\n\n\nspectrum(t, \n         method = \"ar\", log = \"no\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nCalculer le périodogramme et le spectre autorégressif d’une série mensuelle saisonnière.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ns = ts(0, frequency = 12, start = 2000, end = 2020)\ns[cycle(s) == 2] <- 1\nspectrum(s, \n         detrend = FALSE,\n         method = \"pgram\", log = \"no\")\n\n\n\nspectrum(s, \n         method = \"ar\", log = \"no\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nCalculer le périodogramme et le spectre autorégressif d’une cycle de 36 mois.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc = ts(cos(2*pi/36*(1:100)) + sin(2*pi/36*(1:100)), frequency = 12, start = 2000)\nspectrum(c, \n         detrend = FALSE,\n         method = \"pgram\", log = \"no\")\n\n\n\nspectrum(c, \n         method = \"ar\", log = \"no\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nPourquoi a-t-on un spectre différent en simulant un cycle de cette façon ?\n\nc = ts(0, frequency = 12, start = 2000, end = 2020)\nc[cycle(c) == 12][c(TRUE, FALSE, FALSE)] <- 1\nc\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2000   0   0   0   0   0   0   0   0   0   0   0   1\n2001   0   0   0   0   0   0   0   0   0   0   0   0\n2002   0   0   0   0   0   0   0   0   0   0   0   0\n2003   0   0   0   0   0   0   0   0   0   0   0   1\n2004   0   0   0   0   0   0   0   0   0   0   0   0\n2005   0   0   0   0   0   0   0   0   0   0   0   0\n2006   0   0   0   0   0   0   0   0   0   0   0   1\n2007   0   0   0   0   0   0   0   0   0   0   0   0\n2008   0   0   0   0   0   0   0   0   0   0   0   0\n2009   0   0   0   0   0   0   0   0   0   0   0   1\n2010   0   0   0   0   0   0   0   0   0   0   0   0\n2011   0   0   0   0   0   0   0   0   0   0   0   0\n2012   0   0   0   0   0   0   0   0   0   0   0   1\n2013   0   0   0   0   0   0   0   0   0   0   0   0\n2014   0   0   0   0   0   0   0   0   0   0   0   0\n2015   0   0   0   0   0   0   0   0   0   0   0   1\n2016   0   0   0   0   0   0   0   0   0   0   0   0\n2017   0   0   0   0   0   0   0   0   0   0   0   0\n2018   0   0   0   0   0   0   0   0   0   0   0   1\n2019   0   0   0   0   0   0   0   0   0   0   0   0\n2020   0                                            \n\nspectrum(c, \n         method = \"pgram\", log = \"no\")\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCar la série construire reste saisonnière (même valeurs dans les mois autres que décembre) !\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nCalculer le périodogramme d’un bruit blanc (utiliser rnorm()).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(1)\ni = ts(rnorm(100), frequency = 12, start = 2000)\nspectrum(i, \n         detrend = FALSE,\n         method = \"pgram\", log = \"no\")\n\n\n\nspectrum(i, \n         method = \"ar\", log = \"no\")\n\n\n\n\nQuestion supplémentaire : à quoi correspond la droite du graphique précédent ?\nCela correction à \\(\\mathbb V[i]/frequence \\simeq\\) 0,067."
  },
  {
    "objectID": "2022/ast/TP/3_Decomposition.html",
    "href": "2022/ast/TP/3_Decomposition.html",
    "title": "3 - Décomposition d’une série temporelle",
    "section": "",
    "text": "L’objectif de ce TP est d’introduire aux méthodes de décomposition.\n\nDans ce TP nous utiliserons notamment les niveaux bruts des indices de production industrielle (IPI) publiés le 04 novembre 2022 par l’Insee, fichier de données téléchargeable ici : https://www.insee.fr/fr/statistiques/6655844.\nLes packages suivants seront utilisés :\n\npackages_to_install <- c(\"ggplot2\", \"forecast\", \"RJDemetra\", \"ggdemetra\", \"dygraphs\")\n\npackages <- installed.packages()[,\"Package\"][! packages_to_install %in% installed.packages()[,\"Package\"]]\nif (length(packages) > 0) {\n    install.packages(packages)\n}\n\n\n\n\n\n\n\nExercice\n\n\n\nTélécharger et importer les données d’IPI et créer un objet mts qui contient les indices bruts.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nif (!file.exists(\"IPI.xls\")) {\n    download.file(\"https://www.insee.fr/fr/statistiques/fichier/6655844/IPI_202209.xls\", \"IPI.xls\")\n}\nipi <- readxl::read_excel(\"IPI.xls\", sheet = \"niveaux bruts-Raw levels\")\nipi[1, 1]\n\n# A tibble: 1 × 1\n  `NAF rev. 2`\n         <dbl>\n1       199001\n\nipi <- ts(ipi[, -1], start = 1990, frequency = 12)\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nÉtudier les séries de l’IPI dans l’industrie manufacturière (\"CZ\"), dans le textile, habillement, cuir (\"[CB]\") : quelle type de décomposition serait adaptée ? Comparer les résultats de la série désaisonnalisée avec stl(s.window = 7), stl(s.window = \"periodic\") et RJDemetra::x13() avec et sans correction de jours ouvrables (on pourra utiliser la fonction forecast::seasadj() pour extraire la série désaisonnalisée issue de STL et ggdemetra::seasonaladj() pour extraire celle de X-13). Comparer également autour d’une année bissextile.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(forecast)\nlibrary(RJDemetra)\nlibrary(ggdemetra)\nlibrary(ggplot2)\nlibrary(patchwork)\nipi_cz <- ipi[, \"CZ\"]\nipi_cb <- ipi[, \"[CB]\"]\nautoplot(ipi_cz) +\n    labs(y = \"Indice\", title = \"IPI dans l'industrie manufacturière\")\n\n\n\n\nOn repère une tendance à la hausse jusqu’en 2009, une rupture lors de la crise financière puis une tendance qui parait relativement stable. La saisonnalité ne semble pas proportionnelle au niveau, le schéma de décomposition est sûrement additif.\n\ncz_stl_mobile <- stl(ipi_cz, s.window = 7)\ncz_stl_fixed <- stl(ipi_cz, s.window = \"periodic\")\ncz_x13_sans_cjo <- x13(ipi_cz, \"RSA3\")\ncz_x13_avec_cjo <- x13(ipi_cz, \"RSA5c\")\n\n(autoplot(seasadj(cz_stl_fixed), ylab = NULL, main = \"STL saisonnalité fixée\")  +\n        autoplot(seasadj(cz_stl_mobile), ylab = NULL, main = \"STL s.window=7\")) /\n    (autoplot(seasonaladj(cz_x13_sans_cjo), ylab = NULL, main = \"X-13 sans CJO\") +\n        autoplot(seasonaladj(cz_x13_avec_cjo), ylab = NULL, main = \"X-13 avec CJO\"))\n\n\n\n\nLa saisonnalité évolue sûrement dans le temps : STL avec une saisonnalité fixe conduit à des estimations erratiques. Une partie de la variabilité restante provient des effets jours ouvrables.\nIl y a un effet jours ouvrables et un effet année bissextile : la production est plus importante lorsque l’on a un jour de plus dans le mois (ce qui parait logique). Avec STL, l’indice désaisonnalisé devrait donc être sur-estimé en février dans les années bissextiles et sous-estimé les autres années.\n\ncz_x13_avec_cjo$regarima\n\ny = regression model + arima (2, 1, 1, 0, 1, 1)\nLog-transformation: no\nCoefficients:\n          Estimate Std. Error\nPhi(1)      0.0461      0.105\nPhi(2)      0.2134      0.070\nTheta(1)   -0.5000      0.101\nBTheta(1)  -0.6856      0.040\n\n             Estimate Std. Error\nMonday         0.5674      0.223\nTuesday        0.9162      0.222\nWednesday      0.9361      0.222\nThursday       0.1098      0.221\nFriday         0.9240      0.222\nSaturday      -1.6803      0.222\nLeap year      2.2923      0.693\nEaster [1]    -2.3645      0.446\nTC (4-2020)  -20.4000      1.992\nTC (3-2020)  -20.8289      2.002\nAO (5-2011)   12.9309      1.841\nLS (11-2008) -12.3788      1.633\n\n\nResidual standard error: 2.202 on 363 degrees of freedom\nLog likelihood = -843.2, aic =  1720 aicc =  1722, bic(corrected for length) = 1.829\n\nsa_data <- ts.union(seasadj(cz_stl_mobile), seasonaladj(cz_x13_avec_cjo))\ncolnames(sa_data) <- c(\"STL\", \"X13\")\nautoplot(window(sa_data, start = 1995, end = c(1997, 12)),\n         ylab = \"\", \n         main = \"Série désaisonnalisée IPI CZ autour de l'année bissextile de 1996\") + \n    geom_vline(xintercept = c(1996) + 1/12,linetype = 2)\n\n\n\n\nLes coefficients saisonniers semblent surtout évolutifs en juin, août et décembre, ce qui explique également que supposer la saisonnalité stable dans le temps conduit à des estimations bruitées.\n\nplot(cz_x13_avec_cjo$decomposition)\n\n\n\n\nPassons maintenant à la série d’IPI-CB.\n\nautoplot(ipi_cb) + \n    labs(y = \"Indice\", title = \"IPI dans le textile, habillement, cuir (CB)\")\n\n\n\n\nOn repère une tendance à la baisse jusqu’en 2010 puis une tendance relativement stable. La saisonnalité semble proportionnelle au niveau, le schéma de décomposition est sûrement multiplicatif. Il faudra passer au log la série pour STL (schéma de décomposition non géré automatiquement).\n\ncb_stl_mobile <- stl(log(ipi_cb), s.window = 7)\ncb_stl_fixed <- stl(log(ipi_cb), s.window = \"periodic\")\ncb_x13_sans_cjo <- x13(ipi_cb, \"RSA3\")\ncb_x13_avec_cjo <- x13(ipi_cb, \"RSA5c\")\n\np <- (autoplot(exp(seasadj(cb_stl_fixed)), ylab = NULL, main = \"STL saisonnalité fixée\")  + \n        autoplot(exp(seasadj(cb_stl_mobile)), ylab = NULL, main = \"STL s.window=7\")) /\n    (autoplot(seasonaladj(cb_x13_sans_cjo), ylab = NULL, main = \"X-13 sans CJO\") + \n        autoplot(seasonaladj(cb_x13_avec_cjo), ylab = NULL, main = \"X-13 avec CJO\"))\np\n\n\n\n\nIl est difficile d’analyser les graphiques mais il semble que l’on a les mêmes constats que précédemment : STL avec une saisonnalité fixe conduit à des estimations erratiques et une partie de la variabilité restante semble provenir des effets jours ouvrables. On peut refaire le graphique après 2010 en utilisant par exemple la fonction & de patchwork qui permet d’appliquer à élément à tous les graphiques (de la même façon on aurait pu utiliser la fonction window sur les données en entrée).\n\np &  coord_cartesian(xlim = c(2010, 2019),\n                     ylim = c(85, 130))\n\n\n\n\nMême constat que précédemment sur l’année bissextile même si sur l’exemple (1996) la différence n’est pas très forte.\n\ncb_x13_avec_cjo$regarima\n\ny = regression model + arima (3, 1, 1, 0, 1, 0)\nLog-transformation: no\nCoefficients:\n         Estimate Std. Error\nPhi(1)    -0.3753      0.063\nPhi(2)    -0.1075      0.060\nPhi(3)    -0.2599      0.057\nTheta(1)  -0.9478      0.033\n\n             Estimate Std. Error\nMonday         0.9095      0.691\nTuesday        1.0802      0.692\nWednesday      2.3714      0.700\nThursday       0.2554      0.692\nFriday         0.7146      0.701\nSaturday      -2.8858      0.689\nLeap year      1.1978      2.213\nEaster [1]    -5.0870      1.424\nTC (3-2020)  -41.5459      6.539\nLS (1-1996)  -33.9004      5.778\nTC (7-1991)   37.1581      6.258\nLS (2-2004)   25.4803      5.846\nLS (7-1993)   32.3702      5.781\nTC (10-1993) -29.3495      6.286\nLS (4-1997)   26.9281      5.889\nTC (5-1991)  -26.1855      6.274\nAO (4-2020)  -28.3794      6.443\n\n\nResidual standard error:  9.46 on 358 degrees of freedom\nLog likelihood = -1393, aic =  2831 aicc =  2834, bic(corrected for length) = 4.822\n\nsa_data <- ts.union(exp(seasadj(cb_stl_mobile)), seasonaladj(cb_x13_avec_cjo))\ncolnames(sa_data) <- c(\"STL\", \"X13\")\nautoplot(window(sa_data, start = 1995, end = c(1997, 12)),\n         ylab = \"\", \n         main = \"Série désaisonnalisée IPI CB autour de l'année bissextile de 1996\") + \n    geom_vline(xintercept = c(1996) + 1/12,linetype = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nMême exercice sur la série co2 : quel schéma de décomposition parait plausible ? Quel spécification parait adaptée ? On pourra utiliser la fonction le package dygraphs pour comparer les séries.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nautoplot(co2, main = \"Concentration atmosphérique de CO2 à Mauna Loa\")\n\n\n\n\nSérie déjà étudiée : une tendance linéaire mais la saisonnalité n’est pas proportionnelle au niveau. Aucune transformation n’est nécessaire. Pas de raison de faire de la cjo. On repère une tendance à la hausse jusqu’en 2009, une rupture lors de la crise financière puis une tendance qui parait relativement stable. La saisonnalité ne semble pas proportionnelle au niveau, le schéma de décomposition est sûrement additif.\n\nco2_stl_mobile <- stl(co2, s.window = 7)\nco2_stl_fixed <- stl(co2, s.window = \"periodic\")\nco2_x13 <- x13(co2, \"RSA3\")\nco2_x13$regarima # Un schéma multiplicatif est retenu mais cela ne devrait pas changer les résultats\n\ny = regression model + arima (0, 1, 1, 0, 1, 1)\nLog-transformation: yes\nCoefficients:\n          Estimate Std. Error\nTheta(1)   -0.3598      0.044\nBTheta(1)  -0.9116      0.022\n\n\nResidual standard error: 0.0008342 on 452 degrees of freedom\nLog likelihood =  2569, aic = 164.9 aicc = 164.9, bic(corrected for length) = -14.15\n\nco2_x13_add <- x13(co2, \"RSA0\")\ndata_cvs <- ts.union(seasadj(co2_stl_fixed),\n                     seasadj(co2_stl_mobile),\n                     seasonaladj(co2_x13),\n                     seasonaladj(co2_x13_add))\ndata_tc <- ts.union(forecast::trendcycle(co2_stl_fixed),\n                    forecast::trendcycle(co2_stl_mobile),\n                    ggdemetra::trendcycle(co2_x13),\n                    ggdemetra::trendcycle(co2_x13_add))\ndata_s <- ts.union(forecast::seasonal(co2_stl_fixed),\n                   forecast::seasonal(co2_stl_mobile),\n                   ggdemetra::seasonal(co2_x13),\n                   ggdemetra::seasonal(co2_x13_add))\ncolnames(data_cvs) <-  colnames(data_tc) <- colnames(data_s) <-\n    c(\"STL sais. fixée\", \"STL\", \"X-13 (Multiplicatif)\", \"X-13 (Additif)\")\nautoplot(data_cvs, main = \"Série désaisonnalisée\")\n\n\n\nautoplot(data_tc, main = \"Tendance-Cycle\")\n\n\n\nautoplot(data_s, main = \"Composante saisonnière\")\n\n\n\n\nGraphique peu visible mais les résultats semblent proches. Le package dygraphs permet de faire un graphique interactif qui pourra permettre.\n\nlibrary(dygraphs)\ndygraph(data_cvs, main = \"Série désaisonnalisée\") %>%\n    dyRangeSelector()\n\n\n\n\ndygraph(data_tc, main = \"Tendance-Cycle\") %>%\n    dyRangeSelector()\n\n\n\n\n\nEn zoomant on note que les résultats sur la tendance-cycle sont très proches entre les différentes méthodes. Les différences sur la série désaisonnalisée provient essentiellement de la composante saisonnière qui évolue légèrement dans le temps :\n\nfeasts::gg_season(tsibble::as_tsibble(data_s))"
  },
  {
    "objectID": "2022/ast/TP/4_Lissage_Exponentiel.html",
    "href": "2022/ast/TP/4_Lissage_Exponentiel.html",
    "title": "4 - Lissage exponentiel",
    "section": "",
    "text": "L’objectif de ce TP est d’apprendre à appliquer les modèles ETS\n\nLes packages suivants seront utilisés :\n\npackages_to_install <- c(\"ggplot2\", \"forecast\", \"RJDemetra\", \"ggdemetra\")\n\npackages <- installed.packages()[,\"Package\"][! packages_to_install %in% installed.packages()[,\"Package\"]]\nif (length(packages) > 0) {\n    install.packages(packages)\n}\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\n\n\n\n\nExercice\n\n\n\nEtudier les séries co2 et UKgas : quel modèle parait le plus adapté ? Faut-il transformer la série ? Comparer les prévisions en utilisant des schémas additifs et multiplicatifs et en transformant ou non la série.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nautoplot(co2) + autoplot(UKgas)\n\n\n\n\nLes deux séries ont une tendance et une saisonnalité. La saisonnalité parait additive pour co2 et multiplicative pour UKGas. Pas de raison de transformer les séries.\n\nets_co2_add <- ets(co2, model = \"ZZA\")\nets_co2_mult <- ets(co2, model = \"ZZM\")\nets_co2_add\n\nETS(M,A,A) \n\nCall:\n ets(y = co2, model = \"ZZA\") \n\n  Smoothing parameters:\n    alpha = 0.5995 \n    beta  = 0.0065 \n    gamma = 0.129 \n\n  Initial states:\n    l = 315.2927 \n    b = 0.0772 \n    s = -0.8309 -1.8609 -3.0483 -2.782 -1.2615 0.7793\n           2.1909 2.7066 2.1724 1.2282 0.6624 0.0439\n\n  sigma:  9e-04\n\n     AIC     AICc      BIC \n1748.989 1750.349 1819.513 \n\nets_co2_mult\n\nETS(M,Ad,M) \n\nCall:\n ets(y = co2, model = \"ZZM\") \n\n  Smoothing parameters:\n    alpha = 0.6824 \n    beta  = 0.0415 \n    gamma = 3e-04 \n    phi   = 0.9773 \n\n  Initial states:\n    l = 315.3348 \n    b = 0.1077 \n    s = 0.9972 0.9939 0.9904 0.9909 0.9963 1.0024\n           1.0069 1.0088 1.0074 1.004 1.0018 0.9999\n\n  sigma:  8e-04\n\n     AIC     AICc      BIC \n1721.105 1722.628 1795.777 \n\nautoplot(window(co2, start = 1993), y = \"Millions de thermies\") +\n    autolayer(forecast(ets_co2_add, h = 60, PI = FALSE), \"ETS(M,A,A)\") +\n    autolayer(forecast(ets_co2_mult, h = 60, PI = FALSE), \"ETS(M,Ad,M)\")\n\n\n\n\nLes prévisions très proches sur le court terme mais s’éloignent sur le long terme. Cela vient notamment la tendance est amortie dans le modèle multiplicatif.\n\nets_gas_add <- ets(UKgas, model = \"ZZA\")\nets_gas_mult <- ets(UKgas, model = \"ZZM\")\nets_gas_add_log <- ets(UKgas, model = \"ZZA\", lambda = 0)\nets_gas_add_log_unb <- ets(UKgas, model = \"ZZA\", lambda = 0, biasadj = TRUE)\n\nautoplot(window(UKgas, start = 1970), y = \"co2\") +\n    autolayer(forecast(ets_gas_add, h = 24, PI = FALSE), \"ETS(A,A,A)\") +\n    autolayer(forecast(ets_gas_mult, h = 24, PI = FALSE), \"ETS(M,A,M)\") +\n    autolayer(forecast(ets_gas_add_log, h = 24, PI = FALSE), \"ETS(A,A,A) sur log(UKgas)\") \n\n\n\n\nIci la différence entre multiplicatif et additif est plus nette : Dans les deux modèles on prévoit une hausse de la tendance mais les amplitudes croissent de manière exponentielle dans le modèle multiplicatif alors qu’elles restent constantes dans le cas additif (ce qui est logique). Passer au log ne semble pas avoir beaucoup d’impact.\nSur le plus long terme les résultats sont en revanche différents. Passer au log conduit à des estimations plus importantes (notamment lorsque l’on corrige du bais. Rappels : sans correction du biais cela revient à avoir une estimation de la médiane plutôt que moyen, ce qui n’est pas forcément incohérent). C’est logique car lorsque l’on passe au log cela revient à supposer que la tendance est également multiplicative.\n\nautoplot(window(UKgas, start = 1985), y = \"co2\")  +\n    autolayer(forecast(ets_gas_mult, h = 60, PI = FALSE), \"ETS(M,A,M)\") +\n    autolayer(forecast(ets_gas_add_log, h = 60, PI = FALSE), \"ETS(A,A,A) sur log(UKgas)\") +\n    autolayer(forecast(ets_gas_add_log_unb, h = 60, PI = FALSE), \"ETS(A,A,A) sur log(UKgas) corrigé biais\")\n\n\n\n\nAppliquer le modèle \\(ETS(A,A,A)\\) sur la série en logarithme et utiliser le modèle \\(ETS(M,M,M)\\) donnent des résultats similaires.\n\nautoplot(window(UKgas, start = 1985), y = \"co2\")  +\n    autolayer(forecast(ets_gas_add_log, h = 60, PI = FALSE), \"ETS(A,A,A) sur log(UKgas)\") +\n    autolayer(forecast(ets(UKgas, model = \"MMM\"), h = 60, PI = FALSE), \"ETS(M,M,M)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nL’objectif de cette exercice est d’étudier la série AirPassengers en utilisant plusieurs méthodes :\n\nEnlever les 12 derniers mois de AirPassengers.\nDésaisonnaliser la série en utilisant stl(., s.window = \"periodic\") (après transformation de la série) et RJDemetra::x13(., spec = RJDemetra::x13_spec(easter.enabled = FALSE, transform.function = \"Log\")).\nAppliquer un modèle ETS sur la série désaisonnalisée.\nPrévoir la série désaisonnalisée sur 12 mois puis la série brute réintroduisant :\n\n\nla saisonnalité sur la dernière année pour la méthode STL (peut se faire en une étape avec la fonction forecast::stlf()) ;\nles prévisions de la saisonnalité pour la méthode X-13-ARIMA.\n\n\nComparer les prévisions des précédentes méthodes avec un ETS directement calculé sur la série désaisonnalisée (avec ou sans transformation de Box-Cox).\nQu’en est-il de la prévision en temps-réel (en utilisant la fonction tsCV()) ?\n\n\n\n\n\n\n\n\n\nIndice\n\n\n\n\n\nPour récupérer la composante saisonnière avec X-13 on pourra utiliser le code suivant\n\nlibrary(RJDemetra)\ny <- window(ipi_c_eu[,\"FR\"], start = 2010)\nx13_spec <- x13_spec(easter.enabled = FALSE, transform.function = \"Log\")\nmod_x13 <- x13(y, x13_spec)\nggdemetra::seasonal(mod_x13)\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2010 0.9479753 0.9734184 1.1314166 1.0194466 0.9149364 1.1040334 1.0253971\n2011 0.9486036 0.9738929 1.1146702 0.9990949 0.9435734 1.0943653 0.9971672\n2012 0.9744944 0.9952811 1.0894809 0.9994940 0.9609052 1.0773442 1.0308806\n2013 0.9881143 0.9734166 1.0586928 1.0414531 0.9661151 1.0343284 1.0633799\n2014 0.9887445 0.9708317 1.0618924 1.0340629 0.9456969 1.0716857 1.0449424\n2015 0.9611904 0.9664643 1.0947446 1.0268935 0.9248260 1.1112972 1.0452371\n2016 0.9336952 0.9939049 1.1148269 1.0091687 0.9628352 1.0902251 0.9872514\n2017 0.9643035 0.9598990 1.1181909 0.9641950 1.0015167 1.0999538 0.9910340\n2018 0.9940038 0.9589041 1.0890126 0.9941445 0.9936340 1.0782580 1.0244067\n2019 0.9762525 0.9595422 1.0576157 1.0265143 1.0011358 1.0398603 1.0629867\n2020 0.9768492 0.9563783 1.0931460 1.0048644 0.9500369 1.1224476 1.0549500\n           Aug       Sep       Oct       Nov       Dec\n2010 0.7473449 1.0778667 1.0402228 1.0450781 0.9908451\n2011 0.7725295 1.0855555 1.0436457 1.0336021 0.9645023\n2012 0.7668908 1.0190248 1.1114204 1.0328655 0.9392665\n2013 0.7511679 1.0558980 1.0943162 1.0101039 0.9672872\n2014 0.7347217 1.0923548 1.0977123 0.9715733 0.9995163\n2015 0.7410440 1.0774877 1.0685360 1.0097237 0.9855990\n2016 0.7929649 1.0745879 1.0421123 1.0381299 0.9631477\n2017 0.7846862 1.0498446 1.0744776 1.0308174 0.9322877\n2018 0.7891563 1.0084274 1.1117543 1.0382887 0.9294119\n2019 0.7697163 1.0458578 1.0967583 1.0142887 0.9512178\n2020 0.7506828 1.0740572 1.0727587 1.0092022 0.9628865\n\nggdemetra::seasonaladj(mod_x13)\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2010  95.25565  95.64233  96.78133  98.48481 104.37885 101.26505  98.30338\n2011 104.36394 104.32359 103.43866 101.69204 116.68408  99.14422 101.28692\n2012 101.89900 100.37365 101.24088  99.85052 100.00987 100.71062 100.69062\n2013  97.45836  98.31350  98.13990  99.28435  99.57405 101.99855  98.92984\n2014  97.90194 100.01734  98.78591  99.51039  97.70572  97.78987  98.85712\n2015  98.41963  99.43461  99.01853  99.71823  98.39689 100.60315  96.72447\n2016 101.85337 100.21080  97.14512 102.36148 102.40590 101.17176  97.03709\n2017 102.56108 101.15647 102.39754 101.63919 102.54447 101.27698 100.40019\n2018 102.41410 102.82571 103.67189 103.60666  99.33235 104.61318 104.06023\n2019 106.32495 106.30070 105.52037 104.43109 105.08065 101.93677 103.29386\n2020 103.39364 104.66569  83.97780  66.37711  77.57594  87.48738  92.32665\n           Aug       Sep       Oct       Nov       Dec\n2010  99.68623 101.12568 100.93991  98.27017 102.84151\n2011 101.35535 101.33060 101.95031 102.84422 103.88777\n2012 102.75257 100.97889  96.81305  98.65757  97.41645\n2013  97.71450  97.83142  99.87972  98.00971  97.79929\n2014  97.31577  98.13661  98.11314  98.29418  98.44762\n2015 102.69296 101.25406 100.79212 100.81966 101.35969\n2016 100.50886 100.41059  99.22155 100.85443 103.51476\n2017 103.73573 103.06287 105.35353 107.68154 106.72671\n2018 104.66875 103.82503 104.33960 105.55831 105.01264\n2019 102.37538 104.22067 106.22213 102.53491 102.81557\n2020  95.51304  97.48084  99.46319 100.67358 100.32336\n\nggdemetra::seasonal(mod_x13, forecast = TRUE)\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2021 0.9240691 0.9625840 1.1283879 1.0116767 0.9542506 1.1122949 1.0301236\n           Aug       Sep       Oct       Nov       Dec\n2021 0.7736290 1.0661433 1.0421204 1.0454394 0.9628614\n\n\nSi l’on veut une version plus rapide du code on peut également utiliser cette option :\n\nmod_jx13 <- jx13(y, x13_spec)\nsaisonnalite <- get_indicators(mod_jx13, c(\"s\", \"sa\", \"s_f\", \"y_f\"))\nsaisonnalite[[\"s\"]]\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2010 0.9479753 0.9734184 1.1314166 1.0194466 0.9149364 1.1040334 1.0253971\n2011 0.9486036 0.9738929 1.1146702 0.9990949 0.9435734 1.0943653 0.9971672\n2012 0.9744944 0.9952811 1.0894809 0.9994940 0.9609052 1.0773442 1.0308806\n2013 0.9881143 0.9734166 1.0586928 1.0414531 0.9661151 1.0343284 1.0633799\n2014 0.9887445 0.9708317 1.0618924 1.0340629 0.9456969 1.0716857 1.0449424\n2015 0.9611904 0.9664643 1.0947446 1.0268935 0.9248260 1.1112972 1.0452371\n2016 0.9336952 0.9939049 1.1148269 1.0091687 0.9628352 1.0902251 0.9872514\n2017 0.9643035 0.9598990 1.1181909 0.9641950 1.0015167 1.0999538 0.9910340\n2018 0.9940038 0.9589041 1.0890126 0.9941445 0.9936340 1.0782580 1.0244067\n2019 0.9762525 0.9595422 1.0576157 1.0265143 1.0011358 1.0398603 1.0629867\n2020 0.9768492 0.9563783 1.0931460 1.0048644 0.9500369 1.1224476 1.0549500\n           Aug       Sep       Oct       Nov       Dec\n2010 0.7473449 1.0778667 1.0402228 1.0450781 0.9908451\n2011 0.7725295 1.0855555 1.0436457 1.0336021 0.9645023\n2012 0.7668908 1.0190248 1.1114204 1.0328655 0.9392665\n2013 0.7511679 1.0558980 1.0943162 1.0101039 0.9672872\n2014 0.7347217 1.0923548 1.0977123 0.9715733 0.9995163\n2015 0.7410440 1.0774877 1.0685360 1.0097237 0.9855990\n2016 0.7929649 1.0745879 1.0421123 1.0381299 0.9631477\n2017 0.7846862 1.0498446 1.0744776 1.0308174 0.9322877\n2018 0.7891563 1.0084274 1.1117543 1.0382887 0.9294119\n2019 0.7697163 1.0458578 1.0967583 1.0142887 0.9512178\n2020 0.7506828 1.0740572 1.0727587 1.0092022 0.9628865\n\nsaisonnalite[[\"sa\"]]\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2010  95.25565  95.64233  96.78133  98.48481 104.37885 101.26505  98.30338\n2011 104.36394 104.32359 103.43866 101.69204 116.68408  99.14422 101.28692\n2012 101.89900 100.37365 101.24088  99.85052 100.00987 100.71062 100.69062\n2013  97.45836  98.31350  98.13990  99.28435  99.57405 101.99855  98.92984\n2014  97.90194 100.01734  98.78591  99.51039  97.70572  97.78987  98.85712\n2015  98.41963  99.43461  99.01853  99.71823  98.39689 100.60315  96.72447\n2016 101.85337 100.21080  97.14512 102.36148 102.40590 101.17176  97.03709\n2017 102.56108 101.15647 102.39754 101.63919 102.54447 101.27698 100.40019\n2018 102.41410 102.82571 103.67189 103.60666  99.33235 104.61318 104.06023\n2019 106.32495 106.30070 105.52037 104.43109 105.08065 101.93677 103.29386\n2020 103.39364 104.66569  83.97780  66.37711  77.57594  87.48738  92.32665\n           Aug       Sep       Oct       Nov       Dec\n2010  99.68623 101.12568 100.93991  98.27017 102.84151\n2011 101.35535 101.33060 101.95031 102.84422 103.88777\n2012 102.75257 100.97889  96.81305  98.65757  97.41645\n2013  97.71450  97.83142  99.87972  98.00971  97.79929\n2014  97.31577  98.13661  98.11314  98.29418  98.44762\n2015 102.69296 101.25406 100.79212 100.81966 101.35969\n2016 100.50886 100.41059  99.22155 100.85443 103.51476\n2017 103.73573 103.06287 105.35353 107.68154 106.72671\n2018 104.66875 103.82503 104.33960 105.55831 105.01264\n2019 102.37538 104.22067 106.22213 102.53491 102.81557\n2020  95.51304  97.48084  99.46319 100.67358 100.32336\n\nsaisonnalite[[\"s_f\"]]\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2021 0.9240691 0.9625840 1.1283879 1.0116767 0.9542506 1.1122949 1.0301236\n           Aug       Sep       Oct       Nov       Dec\n2021 0.7736290 1.0661433 1.0421204 1.0454394 0.9628614\n\n# On a d'ailleurs directement une prévision de la série brute qui est faite par modèle ARIMA\nsaisonnalite[[\"y_f\"]]\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2021  93.80530  98.83526 116.70635 105.64151  99.21541 117.25615 107.92506\n           Aug       Sep       Oct       Nov       Dec\n2021  81.39014 112.06948 110.18417 110.21518 101.77148\n\n\nPour la fonction tsCV() on pourra utiliser la fonction suivante pour X-13\n\nfx13_stl <- function(x, h){\n    mod_jx13 <- jx13(x, x13_spec)\n    mod_jx13 <- get_indicators(mod_jx13, c(\"sa\", \"s_f\"))\n    ets_x13 <- ets(mod_jx13$sa, model = \"AAN\") # modèle fixé pour gagner du temps\n    ets_x13_f <- forecast(ets_x13, h = h)\n    ets_x13_f$mean <- ets_x13_f$mean * mod_jx13$s_f[1:h] # on ajoute 1:h pour éviter quelques bugs\n    ets_x13_f$model <- \"X-13 + ETS\"\n    # Pas la peine d'actualiser autres paramètres\n    ets_x13_f\n}\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(RJDemetra)\ny <- window(AirPassengers, end = end(AirPassengers) - c(1, 0))\nautoplot(y)\n\n\n\n# Saisonnalité présente qui dépend de la tendance : passage au log nécessaire pour STL\nmod_stl <- stl(log(y), s.window = \"periodic\")\n\nx13_spec <- x13_spec(easter.enabled = FALSE)\nmod_jx13 <- jx13(y, x13_spec)\nmod_jx13 <- get_indicators(mod_jx13, c(\"s\", \"sa\", \"s_f\", \"y_f\"))\nautoplot(exp(seasadj(mod_stl))) + \n    autolayer(mod_jx13$sa)\n\n\n\nets_x13 <- ets(mod_jx13$sa)\nets_stl <- ets(seasadj(mod_stl))\nets_x13_f <- forecast(ets_x13, h = 12)\nets_stl_f <- forecast(ets_stl, h = 12)\nx13_f <- ets_x13_f$mean * mod_jx13$s_f # Il faut multiplier car schéma multiplicatif\nets_f <- ets_stl_f$mean + lag(seasonal(mod_stl), -12)\nets_f <- exp(ets_f)\n\nOn aurait directement pu obtenir les résultats avec fonction forecast::stlf() :\n\nets_f - stlf(y, lambda = 0, h = 12, s.window = \"periodic\")$mean\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1960   0   0   0   0   0   0   0   0   0   0   0   0\n\nest_direct <- ets(y)\nest_direct_bc <- ets(y, lambda = 0)\nautoplot(window(AirPassengers, start = end(AirPassengers) - c(1, 0)),\n         y = \"AirPassengers\") +\n    autolayer(x13_f, series = \"X-13 + ETS\") +\n    autolayer(mod_jx13$y_f, series = \"X-13\")+ \n    autolayer(ets_f, series = \"STL + ETS\")+ \n    autolayer(forecast(est_direct, PI=FALSE, h =12), series = \"ETS(M,Ad,M)\")+ \n    autolayer(forecast(est_direct_bc, PI=FALSE, h =12), series = \"ETS(A,N,A) sur log(AirPassengers)\")\n\n\n\n\nLes séries qui semblent avoir les meilleurs prévisions sont X-13 (modèle ARIMA, voir TP 5) et X-13 + ETS. Le moins bon semble l’ETS directement calculé.\n\nfx13_stl <- function(x, h){\n    mod_jx13 <- jx13(x, x13_spec)\n    mod_jx13 <- get_indicators(mod_jx13, c(\"sa\", \"s_f\"))\n    ets_x13 <- ets(mod_jx13$sa, model = \"AAN\") # modèle fixé pour gagner du temps\n    ets_x13_f <- forecast(ets_x13, h = h)\n    ets_x13_f$mean <- ets_x13_f$mean * mod_jx13$s_f[1:h] # on ajoute 1:h pour éviter quelques bugs\n    ets_x13_f$model <- \"X-13 + ETS\"\n    # Pas la peine d'actualiser autres paramètres\n    # ets_x13_f$upper = ets_x13_f$upper * mod_jx13$s_f\n    # ets_x13_f$lower = ets_x13_f$lower * mod_jx13$s_f\n    ets_x13_f\n}\nfx13 <- function(x, h){\n    mod_jx13 <- jx13(x, x13_spec)\n    x13_f <- get_indicators(mod_jx13, c(\"y_f\"))$y_f\n    ## Ici il y a des calculs en trop car la prévision est uniquement faite avec le pré-ajustement, on pourrait encore optimiser en ne faisant que le pré-ajustement\n    # mod_jx13 <- jregarima(y, x13_spec$regarima)\n    # x13_f <- get_indicators(mod_jx13, c(\"model.y_f\"))[[1]]\n    x13_f <- window(x13_f, end = time(x13_f)[h])\n    return(structure(list(method = \"X-13ARIMA\", \n                          model = mod_jx13, \n                          mean = x13_f,\n                          level = NULL, \n                          lower = NULL, upper = NULL, x = NULL, series = NULL, \n                          fitted = NULL, residuals =NULL), class = \"forecast\"))\n}\nfstl_ets <- function(x, h){\n    stlf(x, lambda = 0, h = 12, s.window = \"periodic\", etsmodel = \"AAN\")\n}\nfets <- function(x, h){\n    forecast(ets(x, model = \"MAM\", damped = TRUE), h = h)\n}\nfets_bc <- function(x, h){\n    forecast(ets(x, lambda = 0, model = \"AAA\"), h = h)\n}\n# On enlève les 3 premières années pour X-13\ne_x13_ets <- tsCV(AirPassengers, fx13_stl, h = 1, initial = 3*12)\ne_x13 <- tsCV(AirPassengers, fx13, h = 1, initial = 3*12)\ne_stl_est <- tsCV(AirPassengers, fstl_ets, h = 1, initial = 3*12)\ne_ets <- tsCV(AirPassengers, fets, h = 1, initial = 3*12)\ne_ets_bc <- tsCV(AirPassengers, fets_bc, h = 1, initial = 3*12)\n\nerreur <- ts.union(e_x13_ets, e_x13, e_stl_est,\n                  e_ets, e_ets_bc)\ncolnames(erreur) <- c(\"X-13+ETS\", \"X-13\", \"STL+ETS\", \"ETS\", \"log(ETS)\")\n\nLe modèle ARIMA (issu de X-13) semble de meilleure qualité :\n\ncolMeans(erreur^2, na.rm = TRUE)\n\nX-13+ETS     X-13  STL+ETS      ETS log(ETS) \n146.4192 140.2659 202.0902 206.2573 210.5955"
  },
  {
    "objectID": "2022/ast/TP/5_ARIMA.html",
    "href": "2022/ast/TP/5_ARIMA.html",
    "title": "5 - Modèles ARIMA",
    "section": "",
    "text": "L’objectif de ce TP est d’apprendre à manipuler des modèles (S)ARIMA\n\nLes modèles ARIMA peuvent être estimés grâce à plusieurs fonctions, sans être exhaustif :\n\nstats::arima() dans les fonctions de base de R ;\nforecast::Arima() basée sur stats::arima() mais qui permet d’ajouter un terme de dérive et se manipule plus facilement avec autres fonctions de forecast ;\nfable::ARIMA() comme forecast::Arima() mais pour les objets tsibble.\n\nLes packages suivants seront utilisés :\n\npackages_to_install <- c(\"ggplot2\", \"forecast\", \"RJDemetra\", \"patchwork\", \"lmtest\",\n                         \"tsibble\", \"fable\", \"feasts\", \"dplyr\", \"lubridate\")\n\npackages <- installed.packages()[,\"Package\"][! packages_to_install %in% installed.packages()[,\"Package\"]]\nif (length(packages) > 0) {\n    install.packages(packages)\n}\n\nLe but des prochains exercices est d’étudier les séries classiques\n\nLakeHuron niveau annuel du Lac de Huron ;\nsunspot.year nombre annuel de tâches solaires entre 1770 et 1869 ;\nAirPassengers nombre mensuel de passagers aériens ;\nnottem température mensuelle moyenne au chateau de Nottingham.\n\n\nNiveau du Lac de Huron\n\n\n\n\n\n\nExercice\n\n\n\nÉtudier la série LakeHuron : Faut-il transformer la série ? Quel modèle ARIMA parait adapté ? la série est elle stationnaire ? Comparer avec auto.arima().\n\n\n\n\n\n\n\n\nIndice\n\n\n\n\n\nAnalyser les ACF/PACF : est-ce qu’ils ressemblent à ceux d’une marche aléatoire ?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(forecast)\nlibrary(patchwork)\nautoplot(LakeHuron)\n\n\n\n\nIl y a potentiellement une tendance à la baisse donc peut-être une tendance à la baisse. A priori pas de raison de transformer la série.\n\ntseries::kpss.test(LakeHuron, \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  LakeHuron\nKPSS Trend = 0.20006, Truncation lag parameter = 3, p-value = 0.01598\n\ntseries::adf.test(LakeHuron)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  LakeHuron\nDickey-Fuller = -2.7796, Lag order = 4, p-value = 0.254\nalternative hypothesis: stationary\n\n# Les tests KPSS et ADF considèrent que la série est non-stationnaire\n# ggAcf(LakeHuron) /\n#   ggPacf(LakeHuron)\nggtsdisplay(LakeHuron)\n\n\n\n\nL’ACF décroit de manière exponentielle et rapidement vers 0, ce n’est pas un signe de marche aléatoire. En revanche le premier coefficient est élevé ce qui peut laisser penser que l’on n’a pas une marche aléatoire mais un coefficient AR(1) élevé. Le PACF est nul à partir de l’ordre 3 : cela peut laisser penser à un processus AR d’ordre au plus 2. On estime un modèle ARIMA(2,0,0) avec une tendance (drift).\n\nmod_trend <- Arima(LakeHuron, order = c(2, 0, 0), include.drift = TRUE)\nmod_trend\n\nSeries: LakeHuron \nARIMA(2,0,0) with drift \n\nCoefficients:\n         ar1      ar2  intercept    drift\n      1.0048  -0.2913   580.0915  -0.0216\ns.e.  0.0976   0.1004     0.4636   0.0081\n\nsigma^2 = 0.476:  log likelihood = -101.2\nAIC=212.4   AICc=213.05   BIC=225.32\n\n# Le coefficient AR(1) est très proche de 1 ce qui explique que les tests précédents concluent à une non-stationnarité\nlmtest::coeftest(mod_trend) # tous les coefficients sont significatifs, pas de raison de simplifier\n\n\nz test of coefficients:\n\n             Estimate  Std. Error   z value  Pr(>|z|)    \nar1         1.0048037   0.0976112   10.2939 < 2.2e-16 ***\nar2        -0.2913198   0.1003652   -2.9026  0.003701 ** \nintercept 580.0915109   0.4635882 1251.3079 < 2.2e-16 ***\ndrift      -0.0215688   0.0080988   -2.6632  0.007740 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# A priori les résidus sont un bruit blanc :\ncheckresiduals(mod_trend) \n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,0) with drift\nQ* = 3.9283, df = 8, p-value = 0.8635\n\nModel df: 2.   Total lags used: 10\n\ncpgram(residuals(mod_trend))\n\n\n\n# En considérant un AR(1) on a un modèle avec un AIC plus grand\nmod_trend_ar1 <- Arima(LakeHuron, order = c(1, 0, 0), include.drift = TRUE)\nmod_trend_ar1\n\nSeries: LakeHuron \nARIMA(1,0,0) with drift \n\nCoefficients:\n         ar1  intercept    drift\n      0.7835   580.0936  -0.0204\ns.e.  0.0634     0.6075   0.0105\n\nsigma^2 = 0.5122:  log likelihood = -105.23\nAIC=218.45   AICc=218.88   BIC=228.79\n\n# On aurait aussi pu faire un ARIMA(0,1,0) : c'est ce qui est retenu par auto.arima()\nmod_diff <- auto.arima(LakeHuron)\nmod_diff\n\nSeries: LakeHuron \nARIMA(0,1,0) \n\nsigma^2 = 0.5588:  log likelihood = -109.11\nAIC=220.22   AICc=220.26   BIC=222.79\n\n\nAttention : on ne peut comparer les modèles en utilisant l’AIC ! (ordre de différenciation différent). Pour comparer les modèles on peut étudier les erreurs de prévision.\n\nfar2 <- function(x, h){forecast(Arima(x, order = c(2, 0, 0), include.drift = TRUE), h = h)}\nfdiff <- function(x, h){forecast(Arima(x, order = c(0, 1, 0)), h = h)}\ne1 <- tsCV(LakeHuron, far2, h = 1)\ne2 <- tsCV(LakeHuron, fdiff, h = 1)\ne_oos <- na.omit(ts.union(e1, e2))\n# MSE plus petite avec second modèle\ncolMeans(e_oos^2)\n\n       e1        e2 \n0.6151966 0.5409957 \n\n# Mais cela vient du fait que lorsqu'il y a peu d'observations, le premier modèle est instable\ncolMeans(window(e_oos,start = 1890)^2)\n\n       e1        e2 \n0.5598778 0.5847280 \n\ncolMeans(window(e_oos,start = 1900)^2)\n\n       e1        e2 \n0.5806291 0.6187056 \n\n# Résidus In Sample toujours plus petits avec premier modèle\n# On commence en 1878 car ce n'est qu'après cette date que les résidus sont calculés par MLE\ne_is <- window(ts.union(residuals(mod_trend), residuals(mod_diff)), start = 1878)\ncolMeans(e_is^2)\n\nresiduals(mod_trend)  residuals(mod_diff) \n           0.4404010            0.5356053 \n\ncolMeans(window(e_is,start = 1890)^2)\n\nresiduals(mod_trend)  residuals(mod_diff) \n           0.4671440            0.5778036 \n\ncolMeans(window(e_is,start = 1900)^2)\n\nresiduals(mod_trend)  residuals(mod_diff) \n           0.4956467            0.6140781 \n\n\n\n\n\n\n\nNombre annuel de tâches solaires\n\n\n\n\n\n\nExercice\n\n\n\nÉtudier la série sunspot.year entre 1770 et 1869 : Faut-il transformer la série ? Quel modèle ARIMA parait adapté ? la série est elle stationnaire ? Comparer avec auto.arima().\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(forecast)\nlibrary(patchwork)\ny <- window(sunspot.year, start = 1770, end = 1869)\nggtsdisplay(y)\n\n\n\n\nPas de tendance ni de raison de transformer la série. Il y a des mouvements cycliques. A priori pas de raison de transformer la série. A priori pas une marche aléatoire.\n\ntseries::kpss.test(y)\n\n\n    KPSS Test for Level Stationarity\n\ndata:  y\nKPSS Level = 0.15928, Truncation lag parameter = 4, p-value = 0.1\n\ntseries::adf.test(y)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  y\nDickey-Fuller = -3.8579, Lag order = 4, p-value = 0.01889\nalternative hypothesis: stationary\n\n\nLes tests KPSS et ADF considèrent que la série est non-stationnaire. On remarque que l’ACF décroit rapidement vers O de manière sinusodiale et que le PACF est nul à partir de l’ordre 3 : on estime un AR2.\n\nmod_ar2 <- Arima(y, order = c(2, 0, 0))\nmod_ar2\n\nSeries: y \nARIMA(2,0,0) with non-zero mean \n\nCoefficients:\n         ar1      ar2     mean\n      1.4059  -0.7111  48.2642\ns.e.  0.0706   0.0702   4.9747\n\nsigma^2 = 236.5:  log likelihood = -414.94\nAIC=837.88   AICc=838.3   BIC=848.3\n\nlmtest::coeftest(mod_ar2) # tous les coefficients sont significatifs, pas de raison de simplifier\n\n\nz test of coefficients:\n\n           Estimate Std. Error  z value  Pr(>|z|)    \nar1        1.405873   0.070572  19.9212 < 2.2e-16 ***\nar2       -0.711095   0.070242 -10.1235 < 2.2e-16 ***\nintercept 48.264166   4.974715   9.7019 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# A priori les résidus sont un bruit blanc :\ncheckresiduals(mod_ar2) \n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,0) with non-zero mean\nQ* = 12.793, df = 8, p-value = 0.1192\n\nModel df: 2.   Total lags used: 10\n\ncpgram(residuals(mod_ar2))\n\n\n\n# Auto.arima sélectionne un ARIMA(2,0,1) qui a un AICc plus petit\nmod_auto <- auto.arima(y)\nmod_auto\n\nSeries: y \nARIMA(2,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ar2     ma1     mean\n      1.2273  -0.5620  0.3733  48.5319\ns.e.  0.1134   0.1084  0.1344   6.0130\n\nsigma^2 = 225.1:  log likelihood = -412.05\nAIC=834.09   AICc=834.73   BIC=847.12\n\naccuracy(mod_ar2)\n\n                     ME     RMSE      MAE  MPE MAPE      MASE      ACF1\nTraining set -0.1065321 15.14691 12.03611 -Inf  Inf 0.7027453 0.1297447\n\naccuracy(mod_auto)\n\n                     ME     RMSE      MAE  MPE MAPE      MASE        ACF1\nTraining set -0.1925486 14.70035 11.89705 -Inf  Inf 0.6946261 -0.02480733\n\nfar2 <- function(x, h){forecast(Arima(x, order=c(2, 0, 0)), h = h)}\nfar2ma1 <- function(x, h){forecast(Arima(x, order=c(2, 0, 1)), h = h)}\ne1 <- tsCV(y, far2, h = 1)\ne2 <- tsCV(y, far2ma1, h = 1)\ne_oos <- window(ts.intersect(e1, e2), start = 1780)\n# MSE plus petite avec second modèle\ncolMeans(e_oos^2, na.rm = TRUE)\n\n      e1       e2 \n217.5813 204.6266 \n\n\n\n\n\n\n\nMombre mensuel de passagers aériens ;\n\n\n\n\n\n\nExercice\n\n\n\nÉtudier la série AirPassengers : Faut-il transformer la série ? Quel modèle ARIMA parait adapté ? la série est elle stationnaire ? Comparer avec auto.arima().\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nautoplot(AirPassengers)\n\n\n\n\nTendance claire avec une saisonnalité multiplicative. Il faut passer la série au log.\n\nggtsdisplay(log(AirPassengers))\n\n\n\n\nL’analyse de l’ACF montre une décroissance lente avec des pics saisonniers. L’analyse du PACF montre deux pics : à l’ordre 1 proche de 1 et à l’ordre 13. Le second pic à l’ordre 13 et non 12 peut suggérer une double différenciation \\((1-B)(1-B^{12})\\). La présence d’une saisonnalité a déjà été analysée dans les précédents TP : un test n’est pas nécessaire et on peut différencier à l’ordre 12.\n\nggtsdisplay(diff(log(AirPassengers), 12))\n\n\n\n\nLa série différenciée ne présente pas de tendance mais des périodes de hausse et de baisse. L’ACF décroit vers 0 mais pas de manière exponentielle. Le premier coefficient de l’ACF/PACF est élevé ce qui peut laisser penser que la série est toujours non-stationnaire\n\ntseries::kpss.test(diff(log(AirPassengers), 12))\n\n\n    KPSS Test for Level Stationarity\n\ndata:  diff(log(AirPassengers), 12)\nKPSS Level = 0.36816, Truncation lag parameter = 4, p-value = 0.09088\n\ntseries::adf.test(diff(log(AirPassengers), 12))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(log(AirPassengers), 12)\nDickey-Fuller = -3.1899, Lag order = 5, p-value = 0.09265\nalternative hypothesis: stationary\n\ntseries::pp.test(diff(log(AirPassengers), 12))\n\n\n    Phillips-Perron Unit Root Test\n\ndata:  diff(log(AirPassengers), 12)\nDickey-Fuller Z(alpha) = -37.597, Truncation lag parameter = 4, p-value\n= 0.01\nalternative hypothesis: stationary\n\n# Les tests KPSS, ADF et PP donnent des résultats différents : à 5 % le test KPSS ne rejette l'hypothèse nulle de stationnarité, le test de PP rejette l'hypothèse de non-stationnarité alors qu'ADF ne la rejette pas.\nndiffs(diff(log(AirPassengers), 12))\n\n[1] 1\n\n\nLa fonction ndiffs(), basée par défaut sur KPSS conclue à une non-stationnarité. Cela vient de paramètres différents dans le test KPSS utilisé. L’analyse des ACF semblent plutôt montrer un présence de marche aléatoire : on différencie. Les ACF et PACF semblent montrer une saisonnalité encore présente mais pas de décroissance nette de l’ACF ou PACF.\n\nggAcf(diff(diff(log(AirPassengers), 12), 1)) /\n    ggPacf(diff(diff(log(AirPassengers), 12), 1))\n\n\n\nmod = Arima(AirPassengers, order = c(0,1,0), seasonal = c(1,1,1), lambda = 0)\nBox.test(resid(mod), fitdf = 2,lag = 24,type = \"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  resid(mod)\nX-squared = 48.603, df = 22, p-value = 0.0009028\n\ncpgram(resid(mod))\n\n\n\n# checkresiduals(mod)\n\nLes résidus ne sont pas un bruit blanc\n\nggAcf(residuals(mod)) /\n    ggPacf(residuals(mod))\n\n\n\n\nEncore pas de décroissance claire mais un pic à l’ordre 1. On peut donc penser que \\(p,q, P,q \\leq 1\\)\n\nmod1 <- Arima(AirPassengers, order = c(1,1,1), seasonal = c(1,1,1), lambda = 0)\n# On a bien un bruit blanc cette fois\nBox.test(resid(mod1), fitdf = 4,lag = 24,type = \"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  resid(mod1)\nX-squared = 24.669, df = 20, p-value = 0.2144\n\ncpgram(resid(mod1))\n\n\n\nlmtest::coeftest(mod1)\n\n\nz test of coefficients:\n\n      Estimate Std. Error z value  Pr(>|z|)    \nar1   0.166649   0.245890  0.6777 0.4979380    \nma1  -0.561499   0.211550 -2.6542 0.0079493 ** \nsar1 -0.099007   0.153981 -0.6430 0.5202347    \nsma1 -0.497321   0.135967 -3.6577 0.0002545 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLes ordres AR ne sont pas significatifs significatifs. On va enlever un ordre AR et refaire le test : ne pas enlever toutes les variables en même temps car on teste ici si une variable est nulle et non pas si un ensemble de variables est nul !\n\nmod2 <- Arima(AirPassengers, order = c(0,1,1), seasonal = c(1,1,1), lambda = 0)\n# Toujours un bruit blanc et AR saisonnier non significatif\nBox.test(resid(mod2), fitdf = 3,lag = 24,type = \"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  resid(mod2)\nX-squared = 25.475, df = 21, p-value = 0.2272\n\nlmtest::coeftest(mod2) \n\n\nz test of coefficients:\n\n      Estimate Std. Error z value  Pr(>|z|)    \nma1  -0.414254   0.089933 -4.6063   4.1e-06 ***\nsar1 -0.111647   0.154748 -0.7215 0.4706159    \nsma1 -0.481706   0.136304 -3.5341 0.0004092 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmod3 = Arima(AirPassengers, order = c(0,1,1), seasonal = c(0,1,1), lambda = 0)\n# Toujours un bruit blanc et tous les coefs sont signifactifs, on ne peut pas simplifier davantage\nBox.test(resid(mod3), fitdf = 2,lag = 24,type = \"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  resid(mod3)\nX-squared = 26.446, df = 22, p-value = 0.233\n\nlmtest::coeftest(mod3) \n\n\nz test of coefficients:\n\n      Estimate Std. Error z value  Pr(>|z|)    \nma1  -0.401828   0.089644 -4.4825 7.378e-06 ***\nsma1 -0.556945   0.073100 -7.6190 2.557e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# C'est le dernier modèle qui a l'AIC le plus petit : c'est celui que l'on retient\nAIC(mod1, mod2, mod3)\n\n     df       AIC\nmod1  5 -480.3109\nmod2  4 -481.9131\nmod3  3 -483.3991\n\n# C'est aussi le modèle retenu par auto.arima\nauto.arima(AirPassengers, lambda = 0)\n\nSeries: AirPassengers \nARIMA(0,1,1)(0,1,1)[12] \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1\n      -0.4018  -0.5569\ns.e.   0.0896   0.0731\n\nsigma^2 = 0.001371:  log likelihood = 244.7\nAIC=-483.4   AICc=-483.21   BIC=-474.77\n\n# auto.arima(AirPassengers, lambda = 0, stepwise = FALSE) # plus lent\n\nOn retrouve le modèle Airline : \\(ARIMA(0,1,1)(0,1,1)\\) !\n\n\n\n\n\nTempérature mensuelle moyenne au chateau de Nottingham\nPour l’analyse de la série nottem, on utilisera le tidyverts. Ci-dessous un exemple de manipulation avec une autre série :\n\nlibrary(tsibble)\nlibrary(dplyr)\nlibrary(fable)\nlibrary(feasts)\nlibrary(ggplot2)\ny <- as_tsibble(USAccDeaths)\ny\n\n# A tsibble: 72 x 2 [1M]\n      index value\n      <mth> <dbl>\n 1 1973 jan  9007\n 2 1973 fév  8106\n 3 1973 mar  8928\n 4 1973 avr  9137\n 5 1973 mai 10017\n 6 1973 jui 10826\n 7 1973 jul 11317\n 8 1973 aoû 10744\n 9 1973 sep  9713\n10 1973 oct  9938\n# … with 62 more rows\n\n(y %>% ACF(value %>%  difference(12)) %>% autoplot()) /\n    (y %>% PACF(value %>%  difference(12)) %>% autoplot()) & ylim(-1,1)\n\n\n\nmodel <- y %>%\n    model(arima = ARIMA(value ~ 0 + pdq(0, 1, 1) + PDQ(0, 1, 0)),\n          auto_arima = ARIMA(value))\nmodel \n\n# A mable: 1 x 2\n                      arima                auto_arima\n                    <model>                   <model>\n1 <ARIMA(0,1,1)(0,1,0)[12]> <ARIMA(0,1,1)(0,1,1)[12]>\n\nmodel %>% accuracy()\n\n# A tibble: 2 × 10\n  .model     .type       ME  RMSE   MAE   MPE  MAPE  MASE RMSSE     ACF1\n  <chr>      <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>\n1 arima      Training  44.0  321.  230. 0.477  2.70 0.526 0.574  0.00648\n2 auto_arima Training  58.8  285.  201. 0.665  2.35 0.460 0.510 -0.0239 \n\nmodel %>% glance()\n\n# A tibble: 2 × 8\n  .model      sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots  \n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl> <list>    <list>    \n1 arima      128002.   -430.  865.  865.  869. <cpl [0]> <cpl [1]> \n2 auto_arima 102860.   -425.  857.  857.  863. <cpl [0]> <cpl [13]>\n\nmodel %>% residuals() %>%  ACF() %>%  autoplot()\n\n\n\n# On peut utiliser la fonction report() sur un sous modèle\nmodel %>% select(auto_arima) %>% \n    report()\n\nSeries: value \nModel: ARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.4303  -0.5528\ns.e.   0.1228   0.1784\n\nsigma^2 estimated as 102860:  log likelihood=-425.44\nAIC=856.88   AICc=857.32   BIC=863.11\n\nmodel %>% \n    forecast(h=12) %>%  \n    autoplot(y)\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nÉtudier la série as_tsibble(nottem) :\n\nFaut-il transformer la série ?\n\nFaut-il différencier la série ? (utiliser la fonction difference())\nÉtudier les ACF/PACF : quels sont les ordre plausibles ?\nTester un ensemble de modèles possibles. Les trier par AICc et prendre celui qui le minimise.\nVérifier la qualité des résidus\nComparer les prévisions avec une sélection automatique et avec un modèle ETS.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(lubridate)\ny <- as_tsibble(nottem)\nautoplot(y, value)\n\n\n\ngg_season(y, value)\n\n\n\n\nSérie déjà étudiée : a priori pas de transformation nécessaire, pas de tendance et saisonnalité mensuelle nette.\n\ny %>% gg_tsdisplay(value %>%  difference(12), plot_type = \"partial\")\n\n\n\n\nA priori série différenciée est stationnaire. L’analyse des ACF/PACF suggère \\(P= 1\\) et/ou \\(Q=1\\), \\(P<=1\\) et \\(Q <= 2\\). Pas de constante dans le modèle.\n\n# Si on ne veut pas écrire tous les codes à la main on peut aussi faire un programme\n# d = expand.grid(p=0:1,q=0:2,P=0:1, Q=0:1)\n# cat(sprintf(\"sarima%i0%i_%i1%i = ARIMA(value ~ -1 + pdq(%i, 0, %i) + PDQ(%i, 1, %i))\",\n#       d$p, d$q, d$P, d$Q,\n#       d$p, d$q, d$P, d$Q), sep =\",\\n\")\nall_models <- y %>%\n    model(\n        sarima000_010 = ARIMA(value ~ -1 + pdq(0, 0, 0) + PDQ(0, 1, 0)),\n        sarima100_010 = ARIMA(value ~ -1 + pdq(1, 0, 0) + PDQ(0, 1, 0)),\n        sarima001_010 = ARIMA(value ~ -1 + pdq(0, 0, 1) + PDQ(0, 1, 0)),\n        sarima101_010 = ARIMA(value ~ -1 + pdq(1, 0, 1) + PDQ(0, 1, 0)),\n        sarima002_010 = ARIMA(value ~ -1 + pdq(0, 0, 2) + PDQ(0, 1, 0)),\n        sarima102_010 = ARIMA(value ~ -1 + pdq(1, 0, 2) + PDQ(0, 1, 0)),\n        sarima000_110 = ARIMA(value ~ -1 + pdq(0, 0, 0) + PDQ(1, 1, 0)),\n        sarima100_110 = ARIMA(value ~ -1 + pdq(1, 0, 0) + PDQ(1, 1, 0)),\n        sarima001_110 = ARIMA(value ~ -1 + pdq(0, 0, 1) + PDQ(1, 1, 0)),\n        sarima101_110 = ARIMA(value ~ -1 + pdq(1, 0, 1) + PDQ(1, 1, 0)),\n        sarima002_110 = ARIMA(value ~ -1 + pdq(0, 0, 2) + PDQ(1, 1, 0)),\n        sarima102_110 = ARIMA(value ~ -1 + pdq(1, 0, 2) + PDQ(1, 1, 0)),\n        sarima000_011 = ARIMA(value ~ -1 + pdq(0, 0, 0) + PDQ(0, 1, 1)),\n        sarima100_011 = ARIMA(value ~ -1 + pdq(1, 0, 0) + PDQ(0, 1, 1)),\n        sarima001_011 = ARIMA(value ~ -1 + pdq(0, 0, 1) + PDQ(0, 1, 1)),\n        sarima101_011 = ARIMA(value ~ -1 + pdq(1, 0, 1) + PDQ(0, 1, 1)),\n        sarima002_011 = ARIMA(value ~ -1 + pdq(0, 0, 2) + PDQ(0, 1, 1)),\n        sarima102_011 = ARIMA(value ~ -1 + pdq(1, 0, 2) + PDQ(0, 1, 1)),\n        sarima000_111 = ARIMA(value ~ -1 + pdq(0, 0, 0) + PDQ(1, 1, 1)),\n        sarima100_111 = ARIMA(value ~ -1 + pdq(1, 0, 0) + PDQ(1, 1, 1)),\n        sarima001_111 = ARIMA(value ~ -1 + pdq(0, 0, 1) + PDQ(1, 1, 1)),\n        sarima101_111 = ARIMA(value ~ -1 + pdq(1, 0, 1) + PDQ(1, 1, 1)),\n        sarima002_111 = ARIMA(value ~ -1 + pdq(0, 0, 2) + PDQ(1, 1, 1)),\n        sarima102_111 = ARIMA(value ~ -1 + pdq(1, 0, 2) + PDQ(1, 1, 1))\n    )\nall_models %>% \n    glance() %>% \n    arrange(AICc)\n\n# A tibble: 24 × 8\n   .model        sigma2 log_lik   AIC  AICc   BIC ar_roots   ma_roots  \n   <chr>          <dbl>   <dbl> <dbl> <dbl> <dbl> <list>     <list>    \n 1 sarima100_111   5.25   -519. 1045. 1045. 1059. <cpl [13]> <cpl [12]>\n 2 sarima002_111   5.23   -518. 1045. 1046. 1063. <cpl [12]> <cpl [14]>\n 3 sarima101_111   5.25   -518. 1046. 1046. 1063. <cpl [13]> <cpl [13]>\n 4 sarima102_111   5.25   -518. 1047. 1048. 1068. <cpl [13]> <cpl [14]>\n 5 sarima001_111   5.33   -520. 1048. 1048. 1062. <cpl [12]> <cpl [13]>\n 6 sarima002_011   5.44   -524. 1055. 1056. 1069. <cpl [0]>  <cpl [14]>\n 7 sarima100_011   5.48   -525. 1056. 1056. 1066. <cpl [1]>  <cpl [12]>\n 8 sarima101_011   5.46   -524. 1056. 1056. 1070. <cpl [1]>  <cpl [13]>\n 9 sarima102_011   5.46   -524. 1057. 1057. 1074. <cpl [1]>  <cpl [14]>\n10 sarima001_011   5.55   -526. 1058. 1058. 1069. <cpl [0]>  <cpl [13]>\n# … with 14 more rows\n\nbest_model <- y %>%\n    model(\n        sarima100_111 = ARIMA(value ~ -1 + pdq(1, 0, 0) + PDQ(1, 1, 1))\n    )\n\nA priori bruit blanc :\n\nbest_model %>% gg_tsresiduals()\n\n\n\naugment(best_model) %>% \n    features(.innov, ljung_box, dof = 3, lag = 24)\n\n# A tibble: 1 × 3\n  .model        lb_stat lb_pvalue\n  <chr>           <dbl>     <dbl>\n1 sarima100_111    20.6     0.484\n\ncompar_model <- y %>%\n    model(\n        sarima100_111 = ARIMA(value ~ -1 + pdq(1, 0, 0) + PDQ(1, 1, 1)),\n        auto_arima = ARIMA(value ~ -1),\n        ets = ETS(value)\n    )\n\nModèle sélectionné a un AICc plus petit que l’auto-arima mais un RMSE plus élevé.\n\ncompar_model\n\n# A mable: 1 x 3\n              sarima100_111                auto_arima          ets\n                    <model>                   <model>      <model>\n1 <ARIMA(1,0,0)(1,1,1)[12]> <ARIMA(1,0,2)(1,1,2)[12]> <ETS(A,N,A)>\n\ncompar_model %>% glance()\n\n# A tibble: 3 × 11\n  .model      sigma2 log_lik   AIC  AICc   BIC ar_ro…¹ ma_ro…²   MSE  AMSE   MAE\n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl> <list>  <list>  <dbl> <dbl> <dbl>\n1 sarima100_…   5.25   -519. 1045. 1045. 1059. <cpl>   <cpl>   NA    NA    NA   \n2 auto_arima    5.23   -517. 1048. 1048. 1072. <cpl>   <cpl>   NA    NA    NA   \n3 ets           5.38   -852. 1735. 1737. 1787. <NULL>  <NULL>   5.07  5.17  1.74\n# … with abbreviated variable names ¹​ar_roots, ²​ma_roots\n\ncompar_model %>% accuracy()\n\n# A tibble: 3 × 10\n  .model        .type         ME  RMSE   MAE    MPE  MAPE  MASE RMSSE     ACF1\n  <chr>         <chr>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl>\n1 sarima100_111 Training 0.0695   2.22  1.72 -0.127  3.69 0.638 0.647 -0.0197 \n2 auto_arima    Training 0.0711   2.20  1.71 -0.124  3.67 0.635 0.641  0.00160\n3 ets           Training 0.00726  2.25  1.74 -0.223  3.75 0.647 0.656  0.198  \n\n\nLes prévisions des 3 modèles sont très proches\n\nforecast(compar_model, h = \"1 year\") %>% \n    autoplot(y %>% filter(year(index) >= 1938))\n\n\n\n\n\n\n\nL’avantage est tidyverts est que l’on peut appliquer facilement plusieurs fonctions à plusieurs séries et comparer les méthodes entre elles. En reprenant l’exemple disponible ici https://fable.tidyverts.org :\n\nlibrary(fable)\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(lubridate)\nlibrary(dplyr)\naus_retail %>%\n    filter(\n        State %in% c(\"New South Wales\", \"Victoria\"),\n        Industry == \"Department stores\"\n    ) %>% \n    model(\n        ets = ETS(box_cox(Turnover, 0.3)),\n        arima = ARIMA(log(Turnover)),\n        snaive = SNAIVE(Turnover)\n    ) %>%\n    forecast(h = \"2 years\") %>% \n    autoplot(filter(aus_retail, year(Month) > 2010), level = NULL)"
  },
  {
    "objectID": "2022/ast/backcasttransform.html",
    "href": "2022/ast/backcasttransform.html",
    "title": "Backcast transformation",
    "section": "",
    "text": "Voir notamment https://robjhyndman.com/hyndsight/backtransforming/.\nCette page vient en complément du cours 2 - Analyse graphique.\n\n1 Convergence vers la médiane\nSoit \\(f_\\lambda\\) la transformation de Box-Cox \\[\nf_\\lambda(x)=\\begin{cases}\n\\log(x)&\\text{ si }\\lambda=0\\\\\n\\frac{sign(x)|x|^\\lambda -1}{\n\\lambda\n}&\\text{ si }\\lambda\\ne0\n\\end{cases}\n\\] et \\(g_\\lambda\\) la transformation inverse \\[\n\\quad\ng_\\lambda(x)=f^{-1}_\\lambda(x) =\n\\begin{cases}\n\\exp(x) & \\text{ si }\\lambda=0\\\\\n\\text{sign}(\\lambda x + 1)|\\lambda x+1|^{1/\\lambda} & \\text{ si }\\lambda\\ne0\n\\end{cases}.\n\\]\nGénéralement la transformation inverse de la valeur prédite ne pourra pas être considérée comme la valeur moyenne de la distribution des prévisions mais plutôt comme la médiane. Il n’y a en général pas de problème à cela mais dans certains cas (comme les prévisions hiérarchiques où l’on fait des agrégations de prévisions) il est nécessaire de faire une correction puisque l’on s’intéresse à la moyenne de la distribution des prévisions (voir section Section 2).\nCette convergence vers la médiane est vraie lorsque la distribution est symétrique puisque dans ce cas : \\[\nMoyenne[f_\\lambda(X)]=Mediane[f_\\lambda(X)]=f_\\lambda(Mediane[X]).\n\\] Il vient donc \\[\ng_\\lambda\\big(Moyenne[f_\\lambda(X)]\\big) =\nMediane\\big[g_\\lambda(f_\\lambda(X))\\big]=\nMediane[X].\n\\]\n\n\n2 Correction du bais avec la transformation Box-Cox\nSoit \\(\\mu\\) la moyenne de la série transformée et \\(\\sigma^{2}\\) sa variance. Les trois premiers termes du développement de Taylor en série entière s’écrivent : \\[\ng_\\lambda(\\mu+x)\\simeq g_\\lambda(\\mu)+g_\\lambda'(\\mu)x+g_\\lambda''(\\mu)\\frac{{x^{2}}}{2}.\n\\]\nIl vient donc : \\[\\begin{align*}\n\\mathbb{E}\\left[g_\\lambda(X)\\right] & =\\mathbb{E}\\left[g_\\lambda(\\mu+X-\\mu)\\right]\\\\\n& \\simeq\\mathbb{E}\\left[g_\\lambda(\\mu)+g_\\lambda'(\\mu)(X-\\mu)+\\frac{1}{2}g_\\lambda''(\\mu)\\left(X-\\mu\\right)^{2}\\right]\\\\\n& =g_\\lambda(\\mu)+g_\\lambda'(\\mu)\\underbrace{\\mathbb{E}\\left[X-\\mu\\right]}_{=0}+\\frac{1}{2}g_\\lambda''(\\mu)\\underbrace{\\mathbb{E}\\left[\\left(X-\\mu\\right)^{2}\\right]}_{=\\sigma^{2}}\\\\\n& =g_\\lambda(\\mu)+g_\\lambda''(\\mu)\\frac{\\sigma^{2}}{2}.\n\\end{align*}\\]\nDe la même façon on peut montrer que \\[\n\\mathbb{V}\\left[g_\\lambda(X)\\right]\\simeq\\left(g_\\lambda'(\\mathbb{E}\\left[X\\right])\\right)^{2}\\mathbb{V}\\left[X\\right]=\\left(g_\\lambda'(\\mu)\\right)^{2}\\sigma^{2}-\\frac{1}{4}\\left(g_\\lambda''(\\mu)\\right)^{2}\\sigma^{4}.\n\\]\nDans le cas de la transformation de Box-Cox \\[\ng_\\lambda(x)=\\begin{cases}\n\\exp(x) & \\text{ si }\\lambda=0\\\\\n\\text{sign}(\\lambda x+1)|\\lambda x+1|^{1/\\lambda} & \\text{ si }\\lambda\\ne0\n\\end{cases},\n\\] donc \\[\ng_\\lambda'(x)=\\begin{cases}\n\\exp(x) & \\text{ si }\\lambda=0\\\\\n\\text{sign}(\\lambda x+1)|\\lambda x+1|^{1/\\lambda-1} & \\text{ si }\\lambda\\ne0\n\\end{cases}\n\\] et \\[\ng_\\lambda''(x)=\\begin{cases}\n\\exp(x) & \\text{ si }\\lambda=0\\\\\n\\text{sign}(\\lambda x+1)(1-\\lambda)|\\lambda x+1|^{1/\\lambda-2} & \\text{ si }\\lambda\\ne0\n\\end{cases}.\n\\]\nLa moyenne de la transformation inverse de Box-Cox est donc \\[\n\\begin{cases}\n\\exp(\\mu)\\left(1+\\frac{\\sigma^{2}}{2}\\right) & \\text{ si }\\lambda=0\\\\\n\\text{sign}(\\lambda\\mu+1)|\\lambda\\mu+1|^{1/\\lambda}\\left(1+\\frac{\\sigma^{2}(1-\\lambda)}{2|\\lambda\\mu+1|^{2}}\\right) & \\text{ si }\\lambda\\ne0\n\\end{cases}.\n\\]\nC’est-à-dire : \\[\n\\begin{cases}\ng_\\lambda(\\mu)\\left(1+\\frac{\\sigma^{2}}{2}\\right) & \\text{ si }\\lambda=0\\\\\ng_\\lambda(\\mu)\\left(1+\\frac{\\sigma^{2}(1-\\lambda)}{2 g_\\lambda(\\mu)^{2\\lambda}}\\right) & \\text{ si }\\lambda\\ne0\n\\end{cases}.\n\\]"
  },
  {
    "objectID": "2022/ast/index.html",
    "href": "2022/ast/index.html",
    "title": "Analyse des séries temporelles avec ",
    "section": "",
    "text": "Supports de cours et exercices de la formation Analyse des séries temporelles avec R au CEPE les 21 et 22 novembre 2022.\n\nCours\n\nRappels sur l’environnement de travail de R\nAnalyse graphique (voir également compléments sur la transformation inverse de Box-Cox)\nDécomposition d’une série temporelle\nLissage exponentiel\nLa modélisation ARIMA\nCompléments\n\n\n\nTravaux pratiques\n\nTraitement des séries temporelles sous R\nAnalyse graphique\nDécomposition d’une série temporelle\nLissage exponentiel\nLa modélisation ARIMA\n\n\n\nBibliographie\nAragon, Y. (2011), Séries temporelles avec R. Méthodes et cas, Springer.\nBrockwell, P. J. and Davis, R. A. (1991) Time Series: Theory and Methods. Second edition. Springer.\nHyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on nov. 2022.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on nov. 2022.\n\n\nLicence\nCes supports de cours sont librement réutilisable sous © 2022 Alain Quartier-la-Tente, Insee CC BY-NC-SA 3.0 ."
  },
  {
    "objectID": "2022/index.html",
    "href": "2022/index.html",
    "title": "Formations",
    "section": "",
    "text": "2022\n\nNovembre 2022 : Analyse de séries temporelles avec  (Cepe)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ensemble des formations",
    "section": "",
    "text": "2022\n\nNovembre 2022 : Analyse de séries temporelles avec  (Cepe)"
  }
]